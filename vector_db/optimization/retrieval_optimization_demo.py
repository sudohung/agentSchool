"""
å‘é‡æ•°æ®åº“æ£€ç´¢å¯é æ€§ä¼˜åŒ–å®æˆ˜ç¤ºä¾‹
Practical Examples for Improving Vector Database Retrieval Reliability
"""

import numpy as np
from sentence_transformers import SentenceTransformer, CrossEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import faiss
import time
import re
from collections import defaultdict
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RetrievalOptimizer:
    """æ£€ç´¢ä¼˜åŒ–å™¨ä¸»ç±»"""
    
    def __init__(self):
        self.models = {}
        self.indexes = {}
        self.quality_metrics = {}
        
    def load_models(self):
        """åŠ è½½å„ç§åµŒå…¥æ¨¡å‹è¿›è¡Œå¯¹æ¯”æµ‹è¯•"""
        logger.info("ğŸš€ åŠ è½½åµŒå…¥æ¨¡å‹...")
        
        model_configs = [
            ('multilingual', 'paraphrase-multilingual-MiniLM-L12-v2'),
            ('english', 'all-MiniLM-L6-v2'),
            ('chinese', 'bge-small-zh-v1.5')
        ]
        
        for name, model_path in model_configs:
            try:
                self.models[name] = SentenceTransformer(model_path)
                logger.info(f"âœ… {name} æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.warning(f"âŒ {name} æ¨¡å‹åŠ è½½å¤±è´¥: {e}")

class ModelComparisonTester:
    """æ¨¡å‹å¯¹æ¯”æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.optimizer = RetrievalOptimizer()
        self.test_data = []
        
    def prepare_test_data(self):
        """å‡†å¤‡æµ‹è¯•æ•°æ®"""
        self.test_data = [
            # æŠ€æœ¯ç±»æŸ¥è¯¢
            ("Pythonç¼–ç¨‹è¯­è¨€çš„ç‰¹ç‚¹", 
             ["Pythonæ˜¯ä¸€ç§è§£é‡Šå‹ã€é¢å‘å¯¹è±¡çš„é«˜çº§ç¼–ç¨‹è¯­è¨€",
              "Pythonè¯­æ³•ç®€æ´æ˜“è¯»ï¼Œé€‚åˆåˆå­¦è€…å­¦ä¹ ",
              "Pythonæ‹¥æœ‰ä¸°å¯Œçš„ç¬¬ä¸‰æ–¹åº“ç”Ÿæ€ç³»ç»Ÿ"]),
            
            ("æœºå™¨å­¦ä¹ ç®—æ³•", 
             ["æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯",
              "å¸¸è§çš„æœºå™¨å­¦ä¹ ç®—æ³•åŒ…æ‹¬å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€ç¥ç»ç½‘ç»œ",
              "ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸»è¦åˆ†ç±»"]),
            
            ("æ·±åº¦å­¦ä¹ æ¡†æ¶", 
             ["TensorFlowæ˜¯Googleå¼€å‘çš„æ·±åº¦å­¦ä¹ æ¡†æ¶",
              "PyTorchç”±Facebookå¼€å‘ï¼Œå¹¿å—ç ”ç©¶è€…æ¬¢è¿",
              "Kerasæ˜¯é«˜å±‚ç¥ç»ç½‘ç»œAPIï¼Œæ˜“äºä½¿ç”¨"]),
            
            # æ—¥å¸¸ç”Ÿæ´»ç±»æŸ¥è¯¢  
            ("å¥åº·é¥®é£Ÿå»ºè®®",
             ["å‡è¡¡é¥®é£ŸåŒ…æ‹¬è›‹ç™½è´¨ã€ç¢³æ°´åŒ–åˆç‰©ã€è„‚è‚ªçš„åˆç†æ­é…",
              "å¤šåƒè”¬èœæ°´æœï¼Œå°‘åƒæ²¹ç‚¸é£Ÿå“",
              "ä¿æŒè§„å¾‹çš„ç”¨é¤æ—¶é—´å’Œé€‚é‡è¿åŠ¨"]),
            
            ("æ—…æ¸¸æ™¯ç‚¹æ¨è",
             ["åŒ—äº¬æ•…å®«æ˜¯ä¸­å›½æ˜æ¸…ä¸¤ä»£çš„çš‡å®¶å®«æ®¿",
              "è¥¿æ¹–ä½äºæ­å·ï¼Œä»¥ç§€ä¸½çš„æ¹–å…‰å±±è‰²é—»å",
              "é•¿åŸæ˜¯ä¸­å›½å¤ä»£çš„å†›äº‹é˜²å¾¡å·¥ç¨‹"])
        ]
        
        logger.info(f"âœ… å‡†å¤‡äº† {len(self.test_data)} ç»„æµ‹è¯•æ•°æ®")
    
    def compare_models(self):
        """å¯¹æ¯”ä¸åŒæ¨¡å‹çš„æ€§èƒ½"""
        logger.info("ğŸ”¬ å¼€å§‹æ¨¡å‹æ€§èƒ½å¯¹æ¯”æµ‹è¯•...")
        
        results = {}
        
        for model_name, model in self.optimizer.models.items():
            logger.info(f"æµ‹è¯•æ¨¡å‹: {model_name}")
            
            model_results = {
                'encoding_times': [],
                'accuracies': [],
                'qualities': []
            }
            
            for query, expected_docs in self.test_data:
                # ç¼–ç æ—¶é—´æµ‹è¯•
                start_time = time.time()
                query_embedding = model.encode([query])
                encoding_time = time.time() - start_time
                model_results['encoding_times'].append(encoding_time)
                
                # è´¨é‡è¯„ä¼°
                doc_embeddings = model.encode(expected_docs)
                similarities = cosine_similarity(query_embedding, doc_embeddings)[0]
                
                # è®¡ç®—å‡†ç¡®æ€§ï¼ˆå‡è®¾æœ€ç›¸å…³çš„åº”è¯¥æ˜¯ç¬¬ä¸€ä¸ªæ–‡æ¡£ï¼‰
                accuracy = similarities[0] if len(similarities) > 0 else 0
                model_results['accuracies'].append(accuracy)
                
                # è´¨é‡åˆ†æ•°è®¡ç®—
                quality_score = self._calculate_quality_score(similarities)
                model_results['qualities'].append(quality_score)
            
            # è®¡ç®—å¹³å‡å€¼
            results[model_name] = {
                'avg_encoding_time': np.mean(model_results['encoding_times']),
                'avg_accuracy': np.mean(model_results['accuracies']),
                'avg_quality': np.mean(model_results['qualities']),
                'std_accuracy': np.std(model_results['accuracies'])
            }
            
            logger.info(f"  å¹³å‡ç¼–ç æ—¶é—´: {results[model_name]['avg_encoding_time']:.4f}s")
            logger.info(f"  å¹³å‡å‡†ç¡®ç‡: {results[model_name]['avg_accuracy']:.3f}")
            logger.info(f"  å¹³å‡è´¨é‡åˆ†: {results[model_name]['avg_quality']:.3f}")
            logger.info("-" * 40)
        
        return results
    
    def _calculate_quality_score(self, similarities):
        """è®¡ç®—è´¨é‡åˆ†æ•°"""
        if len(similarities) < 2:
            return 1.0
        
        # è´¨é‡è¯„ä¼°ï¼šç›¸ä¼¼åº¦åº”è¯¥æœ‰é€‚å½“çš„åŒºåˆ†åº¦
        mean_sim = np.mean(similarities)
        std_sim = np.std(similarities)
        
        # ç†æƒ³æƒ…å†µä¸‹ï¼Œç›¸ä¼¼åº¦æ—¢ä¸è¿‡äºé›†ä¸­ä¹Ÿä¸è¿‡äºåˆ†æ•£
        quality = 1.0 - abs(mean_sim - 0.5) - std_sim
        return max(0, min(1, quality))

class AdvancedPreprocessor:
    """é«˜çº§æ–‡æœ¬é¢„å¤„ç†å™¨"""
    
    def __init__(self):
        self.stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¹Ÿ', 'å¾ˆ', 'è¿˜'}
        self.synonym_map = {
            'äººå·¥æ™ºèƒ½': ['AI', 'äººå·¥æ™ºæ…§'],
            'æœºå™¨å­¦ä¹ ': ['ML', 'æ·±åº¦å­¦ä¹ '],
            'ç¼–ç¨‹': ['å†™ä»£ç ', 'è½¯ä»¶å¼€å‘', 'ç¨‹åºè®¾è®¡'],
            'æ•°æ®': ['èµ„æ–™', 'ä¿¡æ¯'],
            'ç®—æ³•': ['æ¼”ç®—æ³•']
        }
    
    def preprocess_batch(self, texts):
        """æ‰¹é‡é¢„å¤„ç†æ–‡æœ¬"""
        processed_texts = []
        for text in texts:
            processed = self.preprocess_text(text)
            processed_texts.append(processed)
        return processed_texts
    
    def preprocess_text(self, text):
        """å®Œæ•´çš„æ–‡æœ¬é¢„å¤„ç†æµç¨‹"""
        # 1. åŸºç¡€æ¸…ç†
        text = self._basic_clean(text)
        
        # 2. åˆ†è¯å’Œè¿‡æ»¤
        tokens = self._tokenize_and_filter(text)
        
        # 3. åŒä¹‰è¯æ‰©å±•
        expanded_text = self._expand_synonyms(' '.join(tokens))
        
        # 4. æ ‡å‡†åŒ–å¤„ç†
        normalized_text = self._normalize_entities(expanded_text)
        
        return normalized_text.strip()
    
    def _basic_clean(self, text):
        """åŸºç¡€æ–‡æœ¬æ¸…ç†"""
        # ç»Ÿä¸€ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text.strip())
        # å¤„ç†æ ‡ç‚¹ç¬¦å·ï¼ˆä¿ç•™ä¸­è‹±æ–‡æ ‡ç‚¹ï¼‰
        text = re.sub(r'[^\w\s\u4e00-\u9fff\u3000-\u303f\uff00-\uffef]', ' ', text)
        return text
    
    def _tokenize_and_filter(self, text):
        """åˆ†è¯å’Œåœç”¨è¯è¿‡æ»¤"""
        # ç®€å•åˆ†è¯ï¼ˆå®é™…é¡¹ç›®å»ºè®®ä½¿ç”¨jiebaç­‰ä¸“ä¸šå·¥å…·ï¼‰
        tokens = text.split()
        
        # è¿‡æ»¤åœç”¨è¯å’Œè¿‡çŸ­è¯æ±‡
        filtered_tokens = [
            token for token in tokens 
            if len(token.strip()) > 1 and token not in self.stop_words
        ]
        
        return filtered_tokens
    
    def _expand_synonyms(self, text):
        """åŒä¹‰è¯æ‰©å±•"""
        for primary_term, synonyms in self.synonym_map.items():
            if primary_term in text:
                # æ·»åŠ åŒä¹‰è¯åˆ°æ–‡æœ¬ä¸­
                synonym_str = ' '.join(synonyms)
                text = text.replace(primary_term, f"{primary_term} {synonym_str}")
        return text
    
    def _normalize_entities(self, text):
        """å®ä½“æ ‡å‡†åŒ–"""
        # æ—¥æœŸæ ¼å¼æ ‡å‡†åŒ–
        text = re.sub(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', r'\1-\2-\3', text)
        # æ•°å­—æ ¼å¼æ ‡å‡†åŒ–
        text = re.sub(r'(\d+(?:\.\d+)?)\s*(?:ä¸ª|åª|æ¡|ä»½|ä½)', r'\1', text)
        return text

class HybridRetriever:
    """æ··åˆæ£€ç´¢å™¨"""
    
    def __init__(self, preprocessor=None):
        self.preprocessor = preprocessor or AdvancedPreprocessor()
        self.vector_model = None
        self.tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))
        self.vector_index = None
        self.tfidf_index = None
        self.documents = []
        
    def initialize_models(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        try:
            self.vector_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            logger.info("âœ… å‘é‡æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ å‘é‡æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
        return True
    
    def build_indexes(self, documents):
        """æ„å»ºæ··åˆç´¢å¼•"""
        logger.info("ğŸ—ï¸ æ„å»ºæ··åˆæ£€ç´¢ç´¢å¼•...")
        self.documents = documents
        
        # 1. æ–‡æœ¬é¢„å¤„ç†
        processed_docs = self.preprocessor.preprocess_batch(documents)
        
        # 2. æ„å»ºå‘é‡ç´¢å¼•
        if self._build_vector_index(processed_docs):
            logger.info("âœ… å‘é‡ç´¢å¼•æ„å»ºå®Œæˆ")
        
        # 3. æ„å»ºTF-IDFç´¢å¼•
        if self._build_tfidf_index(documents):
            logger.info("âœ… TF-IDFç´¢å¼•æ„å»ºå®Œæˆ")
        
        logger.info(f"âœ… æ€»å…±ç´¢å¼•äº† {len(documents)} ä¸ªæ–‡æ¡£")
    
    def _build_vector_index(self, documents):
        """æ„å»ºå‘é‡ç´¢å¼•"""
        try:
            embeddings = self.vector_model.encode(documents, show_progress_bar=True)
            dimension = embeddings.shape[1]
            
            # ä½¿ç”¨IVFç´¢å¼•æé«˜æœç´¢æ•ˆç‡
            quantizer = faiss.IndexFlatL2(dimension)
            self.vector_index = faiss.IndexIVFFlat(quantizer, dimension, 100)
            self.vector_index.train(embeddings.astype('float32'))
            self.vector_index.add(embeddings.astype('float32'))
            
            return True
        except Exception as e:
            logger.error(f"âŒ å‘é‡ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
            return False
    
    def _build_tfidf_index(self, documents):
        """æ„å»ºTF-IDFç´¢å¼•"""
        try:
            tfidf_matrix = self.tfidf_vectorizer.fit_transform(documents)
            self.tfidf_index = tfidf_matrix
            return True
        except Exception as e:
            logger.error(f"âŒ TF-IDFç´¢å¼•æ„å»ºå¤±è´¥: {e}")
            return False
    
    def hybrid_search(self, query, k=10, weights=None):
        """æ··åˆæœç´¢"""
        if weights is None:
            weights = {'vector': 0.6, 'tfidf': 0.4}
        
        # é¢„å¤„ç†æŸ¥è¯¢
        processed_query = self.preprocessor.preprocess_text(query)
        
        # æ‰§è¡Œä¸åŒç±»å‹çš„æœç´¢
        vector_results = self._vector_search(processed_query, k*2)
        tfidf_results = self._tfidf_search(query, k*2)
        
        # èåˆç»“æœ
        final_results = self._combine_results(
            vector_results, tfidf_results, weights, k
        )
        
        return final_results
    
    def _vector_search(self, query, k):
        """å‘é‡æœç´¢"""
        try:
            query_vector = self.vector_model.encode([query])
            distances, indices = self.vector_index.search(
                query_vector.astype('float32'), k
            )
            
            results = []
            for i, (idx, dist) in enumerate(zip(indices[0], distances[0])):
                if idx < len(self.documents):  # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
                    similarity = 1.0 / (1.0 + dist)
                    results.append((similarity, idx))
            
            return results
        except Exception as e:
            logger.error(f"âŒ å‘é‡æœç´¢å¤±è´¥: {e}")
            return []
    
    def _tfidf_search(self, query, k):
        """TF-IDFæœç´¢"""
        try:
            query_vector = self.tfidf_vectorizer.transform([query])
            similarities = cosine_similarity(query_vector, self.tfidf_index).flatten()
            
            # è·å–top-kç»“æœ
            top_indices = similarities.argsort()[-k:][::-1]
            results = [(similarities[idx], idx) for idx in top_indices 
                      if similarities[idx] > 0]
            
            return results
        except Exception as e:
            logger.error(f"âŒ TF-IDFæœç´¢å¤±è´¥: {e}")
            return []
    
    def _combine_results(self, vector_results, tfidf_results, weights, k):
        """èåˆä¸åŒæœç´¢ç»“æœ"""
        # æ”¶é›†æ‰€æœ‰æ–‡æ¡£IDå’Œå¯¹åº”çš„åˆ†æ•°
        combined_scores = defaultdict(float)
        
        # å¤„ç†å‘é‡æœç´¢ç»“æœ
        for score, doc_id in vector_results:
            combined_scores[doc_id] += weights['vector'] * score
        
        # å¤„ç†TF-IDFæœç´¢ç»“æœ
        for score, doc_id in tfidf_results:
            combined_scores[doc_id] += weights['tfidf'] * score
        
        # æ’åºå¹¶è¿”å›top-k
        sorted_results = sorted(
            combined_scores.items(), 
            key=lambda x: x[1], 
            reverse=True
        )[:k]
        
        # è½¬æ¢ä¸º(åˆ†æ•°, æ–‡æ¡£å†…å®¹)æ ¼å¼
        final_results = [
            (score, self.documents[doc_id]) 
            for doc_id, score in sorted_results
            if doc_id < len(self.documents)
        ]
        
        return final_results

class RetrievalQualityEvaluator:
    """æ£€ç´¢è´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.metrics_history = []
    
    def evaluate_retrieval(self, query, results, ground_truth=None):
        """è¯„ä¼°å•æ¬¡æ£€ç´¢çš„è´¨é‡"""
        evaluation = {
            'query': query,
            'result_count': len(results),
            'timestamp': time.time()
        }
        
        # å¦‚æœæœ‰æ ‡å‡†ç­”æ¡ˆï¼Œè®¡ç®—å‡†ç¡®ç‡æŒ‡æ ‡
        if ground_truth:
            precision, recall = self._calculate_precision_recall(results, ground_truth)
            evaluation.update({
                'precision': precision,
                'recall': recall,
                'f1_score': 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            })
        
        # è®¡ç®—ç»“æœå¤šæ ·æ€§
        diversity_score = self._calculate_diversity(results)
        evaluation['diversity'] = diversity_score
        
        # è®¡ç®—åˆ†æ•°åˆ†å¸ƒ
        if results:
            scores = [score for score, _ in results]
            evaluation.update({
                'avg_score': np.mean(scores),
                'score_std': np.std(scores),
                'score_range': max(scores) - min(scores)
            })
        
        self.metrics_history.append(evaluation)
        return evaluation
    
    def _calculate_precision_recall(self, results, ground_truth):
        """è®¡ç®—ç²¾ç¡®ç‡å’Œå¬å›ç‡"""
        result_docs = [doc for _, doc in results]
        truth_set = set(ground_truth)
        result_set = set(result_docs)
        
        intersection = truth_set & result_set
        precision = len(intersection) / len(result_set) if result_set else 0
        recall = len(intersection) / len(truth_set) if truth_set else 0
        
        return precision, recall
    
    def _calculate_diversity(self, results):
        """è®¡ç®—ç»“æœå¤šæ ·æ€§"""
        if len(results) < 2:
            return 1.0
        
        # ç®€å•çš„å¤šæ ·æ€§è®¡ç®—ï¼šåŸºäºåˆ†æ•°çš„æ ‡å‡†å·®
        scores = [score for score, _ in results]
        score_std = np.std(scores)
        
        # æ ‡å‡†åŒ–åˆ°0-1èŒƒå›´
        max_possible_std = 1.0  # å‡è®¾åˆ†æ•°èŒƒå›´æ˜¯0-1
        diversity = min(1.0, score_std / max_possible_std)
        
        return diversity
    
    def get_overall_metrics(self):
        """è·å–æ•´ä½“æ€§èƒ½æŒ‡æ ‡"""
        if not self.metrics_history:
            return {}
        
        metrics_summary = {
            'total_queries': len(self.metrics_history),
            'avg_result_count': np.mean([m['result_count'] for m in self.metrics_history])
        }
        
        # è®¡ç®—æœ‰æ ‡å‡†ç­”æ¡ˆçš„æŸ¥è¯¢çš„å¹³å‡æŒ‡æ ‡
        evaluated_queries = [m for m in self.metrics_history if 'precision' in m]
        if evaluated_queries:
            metrics_summary.update({
                'avg_precision': np.mean([m['precision'] for m in evaluated_queries]),
                'avg_recall': np.mean([m['recall'] for m in evaluated_queries]),
                'avg_f1': np.mean([m['f1_score'] for m in evaluated_queries]),
                'avg_diversity': np.mean([m['diversity'] for m in self.metrics_history])
            })
        
        return metrics_summary

def run_optimization_demo():
    """è¿è¡Œä¼˜åŒ–æ¼”ç¤º"""
    logger.info("ğŸš€ å¯åŠ¨æ£€ç´¢å¯é æ€§ä¼˜åŒ–æ¼”ç¤º")
    
    # 1. æ¨¡å‹å¯¹æ¯”æµ‹è¯•
    tester = ModelComparisonTester()
    tester.prepare_test_data()
    model_results = tester.compare_models()
    
    logger.info("ğŸ† æ¨¡å‹æ€§èƒ½å¯¹æ¯”ç»“æœ:")
    for model_name, metrics in model_results.items():
        logger.info(f"{model_name}: å‡†ç¡®ç‡={metrics['avg_accuracy']:.3f}, "
                   f"è´¨é‡åˆ†={metrics['avg_quality']:.3f}")
    
    # 2. æ··åˆæ£€ç´¢æ¼”ç¤º
    logger.info("\nğŸ” æ··åˆæ£€ç´¢æ¼”ç¤º:")
    
    # å‡†å¤‡æ¼”ç¤ºæ•°æ®
    demo_documents = [
        "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œä»¥å…¶ç®€æ´æ˜“è¯»çš„è¯­æ³•è€Œé—»å",
        "Javaæ˜¯ä¸€ç§é¢å‘å¯¹è±¡çš„ç¼–ç¨‹è¯­è¨€ï¼Œå…·æœ‰è·¨å¹³å°ç‰¹æ€§",
        "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›é€ æ™ºèƒ½æœºå™¨",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ",
        "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ",
        "TensorFlowæ˜¯Googleå¼€å‘çš„å¼€æºæœºå™¨å­¦ä¹ æ¡†æ¶",
        "PyTorchæ˜¯Facebookå¼€å‘çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå¹¿å—ç ”ç©¶äººå‘˜æ¬¢è¿"
    ]
    
    # åˆ›å»ºæ··åˆæ£€ç´¢å™¨
    hybrid_retriever = HybridRetriever()
    if hybrid_retriever.initialize_models():
        hybrid_retriever.build_indexes(demo_documents)
        
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "Pythonç¼–ç¨‹è¯­è¨€æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
            "æœ‰å“ªäº›æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Ÿ"
        ]
        
        evaluator = RetrievalQualityEvaluator()
        
        for query in test_queries:
            logger.info(f"\nâ“ æŸ¥è¯¢: {query}")
            results = hybrid_retriever.hybrid_search(query, k=3)
            
            logger.info("ğŸ¯ æ£€ç´¢ç»“æœ:")
            for i, (score, doc) in enumerate(results, 1):
                logger.info(f"  {i}. [ç›¸ä¼¼åº¦: {score:.3f}] {doc}")
            
            # è¯„ä¼°è´¨é‡
            evaluation = evaluator.evaluate_retrieval(query, results)
            logger.info(f"ğŸ“Š è´¨é‡è¯„ä¼°: å¤šæ ·æ€§={evaluation['diversity']:.3f}")
    
    # 3. è¾“å‡ºæ•´ä½“æ€§èƒ½æŠ¥å‘Š
    overall_metrics = evaluator.get_overall_metrics()
    logger.info(f"\nğŸ“ˆ æ•´ä½“æ€§èƒ½æŠ¥å‘Š:")
    for metric, value in overall_metrics.items():
        logger.info(f"  {metric}: {value:.3f}")

if __name__ == "__main__":
    run_optimization_demo()