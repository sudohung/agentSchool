# ğŸ¯ å‘é‡æ•°æ®åº“æ£€ç´¢å¯é æ€§ä¼˜åŒ–æŒ‡å—

æå‡å‘é‡æ£€ç´¢çš„å‡†ç¡®æ€§å’Œå¯é æ€§æ˜¯æ„å»ºé«˜è´¨é‡AIåº”ç”¨çš„å…³é”®ã€‚æœ¬æ–‡æ¡£å°†è¯¦ç»†ä»‹ç»å„ç§ä¼˜åŒ–ç­–ç•¥ã€‚

## ğŸ“Š å¯é æ€§é—®é¢˜è¯Šæ–­

### å¸¸è§é—®é¢˜è¯†åˆ«
```python
# è¯Šæ–­å·¥å…·ï¼šè¯„ä¼°æ£€ç´¢è´¨é‡
def diagnose_retrieval_quality(queries, expected_results, actual_results):
    """è¯Šæ–­æ£€ç´¢è´¨é‡é—®é¢˜"""
    metrics = {
        'precision': [],  # ç²¾ç¡®ç‡
        'recall': [],     # å¬å›ç‡  
        'mrr': [],        # å¹³å‡å€’æ•°æ’å
        'ndcg': []        # å½’ä¸€åŒ–æŠ˜æŸç´¯ç§¯å¢ç›Š
    }
    
    for i, (query, expected, actual) in enumerate(zip(queries, expected_results, actual_results)):
        # è®¡ç®—ç²¾ç¡®ç‡
        relevant_count = len(set(expected) & set(actual))
        precision = relevant_count / len(actual) if actual else 0
        metrics['precision'].append(precision)
        
        # è®¡ç®—å¬å›ç‡
        recall = relevant_count / len(expected) if expected else 0
        metrics['recall'].append(recall)
        
        print(f"æŸ¥è¯¢ '{query}':")
        print(f"  ç²¾ç¡®ç‡: {precision:.3f}")
        print(f"  å¬å›ç‡: {recall:.3f}")
        print(f"  æœŸæœ›ç»“æœ: {expected}")
        print(f"  å®é™…ç»“æœ: {actual}")
        print("-" * 40)
    
    return metrics
```

## ğŸ”§ åµŒå…¥æ¨¡å‹ä¼˜åŒ–ç­–ç•¥

### 1. æ¨¡å‹é€‰æ‹©ä¼˜åŒ–
```python
# æ¨¡å‹æ€§èƒ½å¯¹æ¯”æµ‹è¯•
def compare_embedding_models(test_texts):
    """æ¯”è¾ƒä¸åŒåµŒå…¥æ¨¡å‹çš„æ€§èƒ½"""
    models = {
        'paraphrase-multilingual-MiniLM-L12-v2': None,
        'all-MiniLM-L6-v2': None,
        'bge-small-zh-v1.5': None
    }
    
    results = {}
    
    for model_name in models:
        print(f"æµ‹è¯•æ¨¡å‹: {model_name}")
        model = SentenceTransformer(model_name)
        
        # ç¼–ç æµ‹è¯•æ–‡æœ¬
        start_time = time.time()
        embeddings = model.encode(test_texts)
        encoding_time = time.time() - start_time
        
        # è®¡ç®—è´¨é‡æŒ‡æ ‡
        quality_score = evaluate_embedding_quality(embeddings)
        
        results[model_name] = {
            'encoding_time': encoding_time,
            'quality_score': quality_score,
            'dimensions': len(embeddings[0])
        }
        
        print(f"  ç¼–ç æ—¶é—´: {encoding_time:.3f}s")
        print(f"  è´¨é‡å¾—åˆ†: {quality_score:.3f}")
        print(f"  å‘é‡ç»´åº¦: {len(embeddings[0])}")
        print()
    
    return results

def evaluate_embedding_quality(embeddings):
    """è¯„ä¼°åµŒå…¥è´¨é‡"""
    # è®¡ç®—å‘é‡çš„å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦å˜åŒ–
    similarities = []
    for i in range(min(100, len(embeddings))):
        for j in range(i+1, min(100, len(embeddings))):
            sim = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]
            similarities.append(sim)
    
    # è´¨é‡å¾—åˆ†ï¼šé¿å…è¿‡äºé›†ä¸­æˆ–åˆ†æ•£
    mean_sim = np.mean(similarities)
    std_sim = np.std(similarities)
    
    # ç†æƒ³æƒ…å†µä¸‹ï¼Œç›¸ä¼¼åº¦åº”è¯¥æœ‰ä¸€å®šåŒºåˆ†åº¦
    quality = 1.0 - abs(mean_sim - 0.5) - std_sim
    return max(0, quality)
```

### 2. å¾®è°ƒåµŒå…¥æ¨¡å‹
```python
# é¢†åŸŸç‰¹å®šå¾®è°ƒ
def fine_tune_embedding_model(domain_data, base_model='paraphrase-multilingual-MiniLM-L12-v2'):
    """é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„åµŒå…¥æ¨¡å‹å¾®è°ƒ"""
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    train_examples = []
    for text1, text2, label in domain_data:
        train_examples.append(InputExample(texts=[text1, text2], label=label))
    
    # åˆ›å»ºæ¨¡å‹
    model = SentenceTransformer(base_model)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)
    train_loss = losses.CosineSimilarityLoss(model)
    
    # å¾®è°ƒæ¨¡å‹
    model.fit(
        train_objectives=[(train_dataloader, train_loss)],
        epochs=3,
        warmup_steps=100,
        output_path='./fine-tuned-model'
    )
    
    return model
```

## ğŸ—„ï¸ å‘é‡æ•°æ®åº“ä¼˜åŒ–

### 1. ç´¢å¼•ç­–ç•¥ä¼˜åŒ–
```python
# ä¸åŒç´¢å¼•ç±»å‹çš„æ€§èƒ½å¯¹æ¯”
def optimize_index_strategy(vectors, query_vectors):
    """ä¼˜åŒ–ç´¢å¼•ç­–ç•¥"""
    
    dimension = vectors.shape[1]
    results = {}
    
    # æµ‹è¯•ä¸åŒç´¢å¼•ç±»å‹
    index_types = {
        'FlatL2': lambda: faiss.IndexFlatL2(dimension),
        'FlatIP': lambda: faiss.IndexFlatIP(dimension),
        'IVFFlat': lambda: create_ivf_index(dimension, vectors),
        'HNSW': lambda: create_hnsw_index(dimension)
    }
    
    for name, index_creator in index_types.items():
        print(f"æµ‹è¯•ç´¢å¼•ç±»å‹: {name}")
        
        # åˆ›å»ºç´¢å¼•
        index = index_creator()
        if name != 'FlatIP':
            index.add(vectors)
        else:
            # IPç´¢å¼•éœ€è¦å½’ä¸€åŒ–
            normalized_vectors = vectors.copy()
            faiss.normalize_L2(normalized_vectors)
            index.add(normalized_vectors)
        
        # æ€§èƒ½æµ‹è¯•
        start_time = time.time()
        if name != 'FlatIP':
            distances, indices = index.search(query_vectors, 10)
        else:
            normalized_queries = query_vectors.copy()
            faiss.normalize_L2(normalized_queries)
            distances, indices = index.search(normalized_queries, 10)
        
        search_time = time.time() - start_time
        
        results[name] = {
            'search_time': search_time,
            'memory_usage': estimate_memory_usage(index),
            'accuracy': calculate_accuracy(distances)
        }
        
        print(f"  æœç´¢æ—¶é—´: {search_time:.4f}s")
        print(f"  å†…å­˜ä½¿ç”¨: {results[name]['memory_usage']:.2f}MB")
        print()
    
    return results

def create_ivf_index(dimension, vectors, nlist=100):
    """åˆ›å»ºIVFç´¢å¼•"""
    quantizer = faiss.IndexFlatL2(dimension)
    index = faiss.IndexIVFFlat(quantizer, dimension, nlist)
    index.train(vectors)
    return index

def create_hnsw_index(dimension, M=32, efConstruction=200):
    """åˆ›å»ºHNSWç´¢å¼•"""
    index = faiss.IndexHNSWFlat(dimension, M)
    index.hnsw.efConstruction = efConstruction
    return index
```

### 2. æ•°æ®é¢„å¤„ç†ä¼˜åŒ–
```python
# é«˜çº§æ•°æ®é¢„å¤„ç†
class AdvancedPreprocessor:
    """é«˜çº§æ–‡æœ¬é¢„å¤„ç†å™¨"""
    
    def __init__(self):
        self.stop_words = set(['çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº'])
        self.stemmer = None  # å¯ä»¥é›†æˆè¯å¹²æå–å™¨
    
    def preprocess_text(self, text):
        """æ–‡æœ¬é¢„å¤„ç†ç®¡é“"""
        # 1. åŸºç¡€æ¸…ç†
        text = self.basic_clean(text)
        
        # 2. åˆ†è¯å’Œè¿‡æ»¤
        tokens = self.tokenize_and_filter(text)
        
        # 3. å®ä½“è¯†åˆ«å’Œæ ‡å‡†åŒ–
        text = self.normalize_entities(' '.join(tokens))
        
        # 4. åŒä¹‰è¯æ‰©å±•
        text = self.expand_synonyms(text)
        
        return text
    
    def basic_clean(self, text):
        """åŸºç¡€æ–‡æœ¬æ¸…ç†"""
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text.strip())
        # å¤„ç†æ ‡ç‚¹ç¬¦å·
        text = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', text)
        return text
    
    def tokenize_and_filter(self, text):
        """åˆ†è¯å’Œåœç”¨è¯è¿‡æ»¤"""
        # ç®€å•åˆ†è¯ï¼ˆå®é™…é¡¹ç›®ä¸­ä½¿ç”¨jiebaç­‰ä¸“ä¸šåˆ†è¯å·¥å…·ï¼‰
        tokens = text.split()
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        filtered_tokens = [token for token in tokens 
                          if len(token) > 1 and token not in self.stop_words]
        return filtered_tokens
    
    def normalize_entities(self, text):
        """å®ä½“æ ‡å‡†åŒ–"""
        # æ—¥æœŸæ ‡å‡†åŒ–
        text = re.sub(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', r'\1-\2-\3', text)
        # æ•°å­—æ ‡å‡†åŒ–
        text = re.sub(r'(\d+)\s*(?:ä¸ª|åª|æ¡|ä»½)', r'\1', text)
        return text
    
    def expand_synonyms(self, text):
        """åŒä¹‰è¯æ‰©å±•"""
        synonym_dict = {
            'äººå·¥æ™ºèƒ½': 'AI äººå·¥æ™ºæ…§',
            'æœºå™¨å­¦ä¹ ': 'ML æ·±åº¦å­¦ä¹ ',
            'ç¼–ç¨‹': 'å†™ä»£ç  å¼€å‘'
        }
        
        for key, value in synonym_dict.items():
            if key in text:
                text = text.replace(key, f"{key} {value}")
        
        return text
```

## ğŸ¯ æ£€ç´¢ç­–ç•¥ä¼˜åŒ–

### 1. å¤šæ¨¡æ€æ£€ç´¢èåˆ
```python
# å¤šæ¨¡æ€æ£€ç´¢ç³»ç»Ÿ
class MultiModalRetriever:
    """å¤šæ¨¡æ€æ£€ç´¢å™¨"""
    
    def __init__(self):
        self.text_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        self.keyword_index = {}  # å…³é”®è¯å€’æ’ç´¢å¼•
        self.vector_index = None
    
    def build_indexes(self, documents):
        """æ„å»ºå¤šç§ç´¢å¼•"""
        # 1. æ„å»ºå‘é‡ç´¢å¼•
        embeddings = self.text_model.encode(documents)
        dimension = embeddings.shape[1]
        self.vector_index = faiss.IndexFlatL2(dimension)
        self.vector_index.add(embeddings.astype('float32'))
        
        # 2. æ„å»ºå…³é”®è¯ç´¢å¼•
        self._build_keyword_index(documents)
        
        # 3. æ„å»ºæ··åˆç´¢å¼•
        self._build_hybrid_index(documents, embeddings)
    
    def _build_keyword_index(self, documents):
        """æ„å»ºå…³é”®è¯å€’æ’ç´¢å¼•"""
        for doc_id, doc in enumerate(documents):
            words = doc.split()
            for word in words:
                if word not in self.keyword_index:
                    self.keyword_index[word] = set()
                self.keyword_index[word].add(doc_id)
    
    def hybrid_search(self, query, k=10, weights={'vector': 0.7, 'keyword': 0.3}):
        """æ··åˆæœç´¢"""
        # å‘é‡æœç´¢
        vector_scores = self._vector_search(query, k*2)
        
        # å…³é”®è¯æœç´¢
        keyword_scores = self._keyword_search(query, k*2)
        
        # èåˆç»“æœ
        final_scores = self._combine_scores(vector_scores, keyword_scores, weights, k)
        
        return final_scores
    
    def _vector_search(self, query, k):
        """å‘é‡æœç´¢"""
        query_vector = self.text_model.encode([query])
        distances, indices = self.vector_index.search(query_vector.astype('float32'), k)
        
        scores = {}
        for i, (idx, dist) in enumerate(zip(indices[0], distances[0])):
            scores[idx] = 1.0 / (1.0 + dist)  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
        
        return scores
    
    def _keyword_search(self, query, k):
        """å…³é”®è¯æœç´¢"""
        query_words = set(query.split())
        scores = {}
        
        for word in query_words:
            if word in self.keyword_index:
                for doc_id in self.keyword_index[word]:
                    scores[doc_id] = scores.get(doc_id, 0) + 1
        
        # æŒ‰åˆ†æ•°æ’åºå¹¶å–å‰kä¸ª
        sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:k]
        return dict(sorted_scores)
    
    def _combine_scores(self, vector_scores, keyword_scores, weights, k):
        """èåˆä¸åŒæ¥æºçš„åˆ†æ•°"""
        all_doc_ids = set(vector_scores.keys()) | set(keyword_scores.keys())
        combined_scores = {}
        
        for doc_id in all_doc_ids:
            vector_score = vector_scores.get(doc_id, 0)
            keyword_score = keyword_scores.get(doc_id, 0)
            
            # å½’ä¸€åŒ–å¤„ç†
            combined_score = (weights['vector'] * vector_score + 
                            weights['keyword'] * keyword_score)
            combined_scores[doc_id] = combined_score
        
        # æ’åºå¹¶è¿”å›top-k
        sorted_results = sorted(combined_scores.items(), 
                              key=lambda x: x[1], reverse=True)[:k]
        return sorted_results
```

### 2. é‡æ’åºä¼˜åŒ–
```python
# æ™ºèƒ½é‡æ’åºç³»ç»Ÿ
class IntelligentReRanker:
    """æ™ºèƒ½é‡æ’åºå™¨"""
    
    def __init__(self):
        self.cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
    
    def rerank_results(self, query, documents, initial_results, top_k=10):
        """å¯¹åˆå§‹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åº"""
        
        # å‡†å¤‡é‡æ’åºæ•°æ®
        rerank_data = []
        doc_indices = []
        
        for score, doc_idx in initial_results[:top_k*2]:  # å–æ›´å¤šå€™é€‰è¿›è¡Œé‡æ’åº
            rerank_data.append([query, documents[doc_idx]])
            doc_indices.append(doc_idx)
        
        # ä½¿ç”¨äº¤å‰ç¼–ç å™¨è¿›è¡Œç²¾ç»†è¯„åˆ†
        cross_scores = self.cross_encoder.predict(rerank_data)
        
        # ç»“åˆåŸå§‹åˆ†æ•°å’Œäº¤å‰ç¼–ç åˆ†æ•°
        final_scores = []
        for i, (original_score, cross_score) in enumerate(zip(
            [score for score, _ in initial_results[:top_k*2]], cross_scores)):
            # èåˆç­–ç•¥ï¼šåŠ æƒå¹³å‡
            combined_score = 0.3 * original_score + 0.7 * cross_score
            final_scores.append((combined_score, doc_indices[i]))
        
        # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
        final_scores.sort(key=lambda x: x[0], reverse=True)
        
        return final_scores[:top_k]
```

## ğŸ“ˆ è´¨é‡ç›‘æ§å’ŒæŒç»­ä¼˜åŒ–

### 1. å®æ—¶ç›‘æ§ç³»ç»Ÿ
```python
# æ£€ç´¢è´¨é‡ç›‘æ§
class RetrievalMonitor:
    """æ£€ç´¢è´¨é‡ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics_history = {
            'precision': [],
            'recall': [],
            'response_time': [],
            'user_satisfaction': []
        }
    
    def log_query_performance(self, query, results, ground_truth=None, response_time=None):
        """è®°å½•æŸ¥è¯¢æ€§èƒ½"""
        if ground_truth:
            # è®¡ç®—ç²¾åº¦æŒ‡æ ‡
            precision, recall = self._calculate_metrics(results, ground_truth)
            self.metrics_history['precision'].append(precision)
            self.metrics_history['recall'].append(recall)
        
        if response_time:
            self.metrics_history['response_time'].append(response_time)
    
    def _calculate_metrics(self, results, ground_truth):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        result_ids = set([doc_id for _, doc_id in results])
        truth_ids = set(ground_truth)
        
        intersection = result_ids & truth_ids
        precision = len(intersection) / len(result_ids) if result_ids else 0
        recall = len(intersection) / len(truth_ids) if truth_ids else 0
        
        return precision, recall
    
    def get_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report = {}
        for metric, values in self.metrics_history.items():
            if values:
                report[metric] = {
                    'current': values[-1],
                    'average': np.mean(values),
                    'trend': self._calculate_trend(values)
                }
        
        return report
    
    def _calculate_trend(self, values):
        """è®¡ç®—è¶‹åŠ¿"""
        if len(values) < 2:
            return 'insufficient_data'
        
        recent_avg = np.mean(values[-5:])
        overall_avg = np.mean(values)
        
        if recent_avg > overall_avg * 1.1:
            return 'improving'
        elif recent_avg < overall_avg * 0.9:
            return 'degrading'
        else:
            return 'stable'
```

### 2. A/Bæµ‹è¯•æ¡†æ¶
```python
# æ£€ç´¢ç­–ç•¥A/Bæµ‹è¯•
class RetrievalABTester:
    """æ£€ç´¢ç­–ç•¥A/Bæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.variants = {}
        self.test_results = {}
    
    def add_variant(self, name, retrieval_function):
        """æ·»åŠ æµ‹è¯•å˜ä½“"""
        self.variants[name] = retrieval_function
        self.test_results[name] = {
            'queries': [],
            'metrics': []
        }
    
    def run_test(self, test_queries, ground_truth, duration_hours=24):
        """è¿è¡ŒA/Bæµ‹è¯•"""
        start_time = time.time()
        end_time = start_time + (duration_hours * 3600)
        
        query_index = 0
        variant_names = list(self.variants.keys())
        
        while time.time() < end_time and query_index < len(test_queries):
            # è½®æµæµ‹è¯•ä¸åŒå˜ä½“
            variant_name = variant_names[query_index % len(variant_names)]
            query = test_queries[query_index]
            truth = ground_truth[query_index]
            
            # æ‰§è¡Œæ£€ç´¢
            start_query = time.time()
            results = self.variants[variant_name](query)
            response_time = time.time() - start_query
            
            # è®°å½•ç»“æœ
            precision, recall = self._calculate_accuracy(results, truth)
            
            self.test_results[variant_name]['queries'].append({
                'query': query,
                'response_time': response_time,
                'precision': precision,
                'recall': recall
            })
            
            query_index += 1
        
        return self.analyze_results()
    
    def analyze_results(self):
        """åˆ†ææµ‹è¯•ç»“æœ"""
        analysis = {}
        
        for variant_name, data in self.test_results.items():
            if not data['queries']:
                continue
                
            precisions = [q['precision'] for q in data['queries']]
            recalls = [q['recall'] for q in data['queries']]
            response_times = [q['response_time'] for q in data['queries']]
            
            analysis[variant_name] = {
                'avg_precision': np.mean(precisions),
                'avg_recall': np.mean(recalls),
                'avg_response_time': np.mean(response_times),
                'precision_std': np.std(precisions),
                'sample_size': len(data['queries'])
            }
        
        return analysis
```

## ğŸ›¡ï¸ å¼‚å¸¸å¤„ç†å’Œå®¹é”™æœºåˆ¶

```python
# å¥å£®çš„æ£€ç´¢ç³»ç»Ÿ
class RobustRetrievalSystem:
    """å¥å£®çš„æ£€ç´¢ç³»ç»Ÿ"""
    
    def __init__(self, fallback_strategies=None):
        self.primary_retriever = None
        self.fallback_retrievers = fallback_strategies or []
        self.error_log = []
    
    def safe_search(self, query, k=10):
        """å®‰å…¨çš„æœç´¢æ“ä½œ"""
        try:
            # å°è¯•ä¸»è¦æ£€ç´¢å™¨
            results = self.primary_retriever.search(query, k)
            
            # éªŒè¯ç»“æœè´¨é‡
            if self._validate_results(results):
                return results
            else:
                raise Exception("ç»“æœè´¨é‡ä¸è¾¾æ ‡")
                
        except Exception as e:
            self._log_error(f"ä¸»æ£€ç´¢å™¨å¤±è´¥: {str(e)}")
            return self._execute_fallback(query, k)
    
    def _execute_fallback(self, query, k):
        """æ‰§è¡Œå¤‡ç”¨ç­–ç•¥"""
        for i, fallback in enumerate(self.fallback_retrievers):
            try:
                results = fallback.search(query, k)
                if self._validate_results(results):
                    self._log_error(f"å¤‡ç”¨ç­–ç•¥ {i+1} æˆåŠŸ")
                    return results
            except Exception as e:
                self._log_error(f"å¤‡ç”¨ç­–ç•¥ {i+1} å¤±è´¥: {str(e)}")
                continue
        
        # æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»“æœ
        self._log_error("æ‰€æœ‰æ£€ç´¢ç­–ç•¥å¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»“æœ")
        return self._get_default_results(query, k)
    
    def _validate_results(self, results):
        """éªŒè¯ç»“æœè´¨é‡"""
        if not results:
            return False
        
        # æ£€æŸ¥ç»“æœå¤šæ ·æ€§
        if len(set(results)) < len(results) * 0.5:
            return False
        
        # æ£€æŸ¥ç»“æœç›¸å…³æ€§ï¼ˆå¦‚æœæœ‰åˆ†æ•°çš„è¯ï¼‰
        scores = [score for score, _ in results[:5]]
        if len(scores) > 1 and np.std(scores) < 0.1:
            return False
        
        return True
    
    def _get_default_results(self, query, k):
        """è·å–é»˜è®¤ç»“æœ"""
        # å¯ä»¥è¿”å›çƒ­é—¨æ–‡æ¡£æˆ–éšæœºæ–‡æ¡£
        return [(0.5, i) for i in range(min(k, 100))]
    
    def _log_error(self, message):
        """è®°å½•é”™è¯¯"""
        error_entry = {
            'timestamp': time.time(),
            'message': message
        }
        self.error_log.append(error_entry)
        
        # é™åˆ¶æ—¥å¿—å¤§å°
        if len(self.error_log) > 1000:
            self.error_log = self.error_log[-500:]
```

## ğŸ“Š æœ€ä½³å®è·µæ€»ç»“

### çŸ­æœŸä¼˜åŒ–ï¼ˆç«‹å³å¯å®æ–½ï¼‰
1. âœ… é€‰æ‹©åˆé€‚çš„åµŒå…¥æ¨¡å‹
2. âœ… ä¼˜åŒ–ç´¢å¼•å‚æ•°
3. âœ… å®æ–½åŸºç¡€çš„æ•°æ®é¢„å¤„ç†
4. âœ… æ·»åŠ ç®€å•çš„é‡æ’åºæœºåˆ¶

### ä¸­æœŸä¼˜åŒ–ï¼ˆ1-2å‘¨ï¼‰
1. âœ… å®æ–½æ··åˆæ£€ç´¢ç­–ç•¥
2. âœ… å»ºç«‹è´¨é‡ç›‘æ§ç³»ç»Ÿ
3. âœ… è¿›è¡ŒA/Bæµ‹è¯•
4. âœ… å®æ–½å¼‚å¸¸å¤„ç†æœºåˆ¶

### é•¿æœŸä¼˜åŒ–ï¼ˆ1ä¸ªæœˆä»¥ä¸Šï¼‰
1. âœ… é¢†åŸŸç‰¹å®šæ¨¡å‹å¾®è°ƒ
2. âœ… é«˜çº§é¢„å¤„ç†ç®¡é“
3. âœ… æŒç»­å­¦ä¹ å’Œè‡ªé€‚åº”ä¼˜åŒ–
4. âœ… å¤šæ¨¡æ€èåˆæ£€ç´¢

è®°ä½ï¼šä¼˜åŒ–æ˜¯ä¸€ä¸ªæŒç»­çš„è¿‡ç¨‹ï¼Œéœ€è¦æ ¹æ®å®é™…ä½¿ç”¨æƒ…å†µè¿›è¡Œè°ƒæ•´ï¼