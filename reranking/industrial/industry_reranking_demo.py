"""
å·¥ä¸šçº§é‡æ’ä¼˜åŒ–å®æˆ˜æ¼”ç¤º
Industry-Grade Reranking Optimization Demo
"""

import numpy as np
from sentence_transformers import SentenceTransformer, CrossEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
import faiss
import time
import logging
from typing import List, Dict, Tuple, Any
from dataclasses import dataclass
from abc import ABC, abstractmethod

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class SearchResult:
    """æœç´¢ç»“æœæ•°æ®ç»“æ„"""
    doc_id: int
    content: str
    initial_score: float
    features: Dict[str, float]

@dataclass  
class RerankRequest:
    """é‡æ’è¯·æ±‚æ•°æ®ç»“æ„"""
    query: str
    candidates: List[SearchResult]
    context: Dict[str, Any] = None

class BaseReranker(ABC):
    """é‡æ’å™¨åŸºç±»"""
    
    @abstractmethod
    def rerank(self, request: RerankRequest) -> List[SearchResult]:
        pass

class PointwiseReranker(BaseReranker):
    """Pointwiseé‡æ’å™¨ - å·¥ä¸šç•Œæœ€å¸¸ç”¨"""
    
    def __init__(self, model_name: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"):
        self.model = CrossEncoder(model_name)
        logger.info(f"âœ… Pointwiseé‡æ’å™¨åˆå§‹åŒ–å®Œæˆ: {model_name}")
    
    def rerank(self, request: RerankRequest) -> List[SearchResult]:
        """åŸºäºäº¤å‰ç¼–ç å™¨çš„pointwiseé‡æ’"""
        if not request.candidates:
            return []
        
        # æ„å»ºæŸ¥è¯¢-æ–‡æ¡£å¯¹
        query_doc_pairs = [[request.query, candidate.content] 
                          for candidate in request.candidates]
        
        # é¢„æµ‹ç›¸å…³æ€§åˆ†æ•°
        start_time = time.time()
        scores = self.model.predict(query_doc_pairs, show_progress_bar=False)
        inference_time = time.time() - start_time
        
        # æ›´æ–°åˆ†æ•°å¹¶æ’åº
        reranked_candidates = []
        for i, (candidate, score) in enumerate(zip(request.candidates, scores)):
            new_candidate = SearchResult(
                doc_id=candidate.doc_id,
                content=candidate.content,
                initial_score=float(score),
                features={**candidate.features, 'cross_encoder_score': float(score)}
            )
            reranked_candidates.append(new_candidate)
        
        # æŒ‰åˆ†æ•°é™åºæ’åˆ—
        reranked_candidates.sort(key=lambda x: x.initial_score, reverse=True)
        
        logger.info(f"ğŸ”„ Pointwiseé‡æ’å®Œæˆï¼Œå¤„ç†{len(reranked_candidates)}ä¸ªå€™é€‰ï¼Œè€—æ—¶{inference_time:.4f}ç§’")
        return reranked_candidates

class EnsembleReranker(BaseReranker):
    """é›†æˆé‡æ’å™¨ - å¤šæ¨¡å‹èåˆ"""
    
    def __init__(self):
        self.pointwise_reranker = PointwiseReranker()
        self.semantic_reranker = SemanticReranker()
        self.feature_based_reranker = FeatureBasedReranker()
        logger.info("âœ… é›†æˆé‡æ’å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def rerank(self, request: RerankRequest) -> List[SearchResult]:
        """å¤šæ¨¡å‹é›†æˆé‡æ’"""
        # è·å–å„ä¸ªæ¨¡å‹çš„æ’åºç»“æœ
        pointwise_results = self.pointwise_reranker.rerank(request)
        semantic_results = self.semantic_reranker.rerank(request)
        feature_results = self.feature_based_reranker.rerank(request)
        
        # èåˆå¤šä¸ªæ’åºç»“æœ
        fused_results = self._fuse_rankings([
            pointwise_results,
            semantic_results, 
            feature_results
        ], weights=[0.5, 0.3, 0.2])
        
        logger.info(f"ğŸ”„ é›†æˆé‡æ’å®Œæˆï¼Œèåˆäº†3ä¸ªæ¨¡å‹çš„ç»“æœ")
        return fused_results
    
    def _fuse_rankings(self, rankings_list: List[List[SearchResult]], 
                      weights: List[float]) -> List[SearchResult]:
        """èåˆå¤šä¸ªæ’åºç»“æœ"""
        if not rankings_list or not weights:
            return []
        
        # æ”¶é›†æ‰€æœ‰æ–‡æ¡£ID
        all_doc_ids = set()
        for ranking in rankings_list:
            all_doc_ids.update(candidate.doc_id for candidate in ranking)
        
        # è®¡ç®—åŠ æƒèåˆåˆ†æ•°
        fused_scores = {}
        doc_mapping = {}
        
        for ranking, weight in zip(rankings_list, weights):
            ranking_length = len(ranking)
            for rank_pos, candidate in enumerate(ranking):
                doc_id = candidate.doc_id
                # ä½¿ç”¨æ’åå€’æ•°ä½œä¸ºåˆ†æ•°ï¼ˆæ’åè¶Šå‰åˆ†æ•°è¶Šé«˜ï¼‰
                score = (ranking_length - rank_pos) * weight
                fused_scores[doc_id] = fused_scores.get(doc_id, 0) + score
                doc_mapping[doc_id] = candidate
        
        # æŒ‰èåˆåˆ†æ•°æ’åº
        sorted_doc_ids = sorted(fused_scores.keys(), 
                               key=lambda x: fused_scores[x], 
                               reverse=True)
        
        # æ„å»ºæœ€ç»ˆç»“æœ
        final_results = [doc_mapping[doc_id] for doc_id in sorted_doc_ids]
        return final_results

class SemanticReranker(BaseReranker):
    """è¯­ä¹‰é‡æ’å™¨"""
    
    def __init__(self):
        self.encoder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        logger.info("âœ… è¯­ä¹‰é‡æ’å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def rerank(self, request: RerankRequest) -> List[SearchResult]:
        """åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„é‡æ’"""
        if not request.candidates:
            return []
        
        # ç¼–ç æŸ¥è¯¢å’Œå€™é€‰æ–‡æ¡£
        query_embedding = self.encoder.encode([request.query])
        doc_embeddings = self.encoder.encode([c.content for c in request.candidates])
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = np.dot(doc_embeddings, query_embedding.T).flatten()
        
        # æ›´æ–°å€™é€‰ç»“æœ
        reranked_candidates = []
        for i, (candidate, similarity) in enumerate(zip(request.candidates, similarities)):
            new_candidate = SearchResult(
                doc_id=candidate.doc_id,
                content=candidate.content,
                initial_score=float(similarity),
                features={**candidate.features, 'semantic_similarity': float(similarity)}
            )
            reranked_candidates.append(new_candidate)
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        reranked_candidates.sort(key=lambda x: x.initial_score, reverse=True)
        
        return reranked_candidates

class FeatureBasedReranker(BaseReranker):
    """åŸºäºç‰¹å¾çš„é‡æ’å™¨"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.model = RandomForestRegressor(n_estimators=100, random_state=42)
        self.is_trained = False
        logger.info("âœ… ç‰¹å¾é‡æ’å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def train(self, training_data: List[Tuple[RerankRequest, List[int]]]):
        """è®­ç»ƒç‰¹å¾é‡æ’æ¨¡å‹"""
        logger.info("ğŸ‹ï¸ å¼€å§‹è®­ç»ƒç‰¹å¾é‡æ’æ¨¡å‹...")
        
        X, y = [], []
        
        for request, ground_truth_ranks in training_data:
            # ä¸ºæ¯ä¸ªå€™é€‰æ–‡æ¡£æå–ç‰¹å¾
            for i, candidate in enumerate(request.candidates):
                features = self._extract_features(request.query, candidate)
                X.append(list(features.values()))
                
                # è®¡ç®—æ ‡ç­¾ï¼ˆæ’åä½ç½®çš„å€’æ•°ï¼‰
                rank_position = ground_truth_ranks.index(candidate.doc_id) \
                    if candidate.doc_id in ground_truth_ranks else len(ground_truth_ranks)
                y.append(1.0 / (rank_position + 1))  # è½¬æ¢ä¸ºåˆ†æ•°
        
        if X and y:
            # æ ‡å‡†åŒ–ç‰¹å¾
            X_scaled = self.scaler.fit_transform(X)
            # è®­ç»ƒæ¨¡å‹
            self.model.fit(X_scaled, y)
            self.is_trained = True
            logger.info("âœ… ç‰¹å¾é‡æ’æ¨¡å‹è®­ç»ƒå®Œæˆ")
        else:
            logger.warning("âš ï¸ è®­ç»ƒæ•°æ®ä¸è¶³ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹")
    
    def rerank(self, request: RerankRequest) -> List[SearchResult]:
        """åŸºäºæœºå™¨å­¦ä¹ ç‰¹å¾çš„é‡æ’"""
        if not request.candidates:
            return []
        
        if not self.is_trained:
            logger.warning("âš ï¸ æ¨¡å‹æœªè®­ç»ƒï¼Œä½¿ç”¨é»˜è®¤æ’åº")
            return request.candidates
        
        # æå–ç‰¹å¾
        features_list = []
        for candidate in request.candidates:
            features = self._extract_features(request.query, candidate)
            features_list.append(list(features.values()))
        
        # é¢„æµ‹åˆ†æ•°
        features_scaled = self.scaler.transform(features_list)
        predicted_scores = self.model.predict(features_scaled)
        
        # æ›´æ–°å€™é€‰ç»“æœ
        reranked_candidates = []
        for i, (candidate, score) in enumerate(zip(request.candidates, predicted_scores)):
            new_candidate = SearchResult(
                doc_id=candidate.doc_id,
                content=candidate.content,
                initial_score=float(score),
                features={**candidate.features, 'ml_score': float(score)}
            )
            reranked_candidates.append(new_candidate)
        
        # æŒ‰é¢„æµ‹åˆ†æ•°æ’åº
        reranked_candidates.sort(key=lambda x: x.initial_score, reverse=True)
        
        return reranked_candidates
    
    def _extract_features(self, query: str, candidate: SearchResult) -> Dict[str, float]:
        """æå–é‡æ’ç‰¹å¾"""
        features = {}
        
        # æ–‡æœ¬é•¿åº¦ç‰¹å¾
        features['query_length'] = len(query)
        features['doc_length'] = len(candidate.content)
        features['length_ratio'] = len(candidate.content) / (len(query) + 1)
        
        # è¯æ±‡åŒ¹é…ç‰¹å¾
        query_words = set(query.lower().split())
        doc_words = set(candidate.content.lower().split())
        features['word_overlap'] = len(query_words & doc_words)
        features['overlap_ratio'] = len(query_words & doc_words) / (len(query_words) + 1)
        
        # BM25å¯å‘å¼ç‰¹å¾
        features['bm25_like'] = self._bm25_heuristic(query, candidate.content)
        
        # åŸå§‹åˆ†æ•°ç‰¹å¾
        features['initial_score'] = candidate.initial_score
        
        # ä¸Šä¸‹æ–‡ç‰¹å¾ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if candidate.features:
            features.update(candidate.features)
        
        return features
    
    def _bm25_heuristic(self, query: str, doc: str) -> float:
        """BM25å¯å‘å¼è®¡ç®—"""
        query_terms = query.lower().split()
        doc_terms = doc.lower().split()
        
        k1, b = 1.2, 0.75
        avg_doc_len = len(doc_terms)
        doc_len = len(doc_terms)
        
        score = 0
        for term in query_terms:
            tf = doc_terms.count(term)
            if tf > 0:
                idf = np.log((1000 - tf + 0.5) / (tf + 0.5))
                numerator = tf * (k1 + 1)
                denominator = tf + k1 * (1 - b + b * doc_len / avg_doc_len)
                score += idf * numerator / denominator
        
        return score

class OnlineLearningReranker(BaseReranker):
    """åœ¨çº¿å­¦ä¹ é‡æ’å™¨"""
    
    def __init__(self, base_reranker: BaseReranker):
        self.base_reranker = base_reranker
        self.feedback_buffer = []
        self.update_threshold = 100  # æ”¶é›†100ä¸ªåé¦ˆåæ›´æ–°
        logger.info("âœ… åœ¨çº¿å­¦ä¹ é‡æ’å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def rerank(self, request: RerankRequest) -> List[SearchResult]:
        """å¸¦åœ¨çº¿å­¦ä¹ çš„é‡æ’"""
        # ä½¿ç”¨åŸºç¡€é‡æ’å™¨
        results = self.base_reranker.rerank(request)
        
        # è®°å½•ç”¨äºåœ¨çº¿å­¦ä¹ çš„ä¿¡æ¯
        self._record_interaction(request, results)
        
        return results
    
    def record_feedback(self, query_id: str, clicked_doc_ids: List[int]):
        """è®°å½•ç”¨æˆ·åé¦ˆ"""
        self.feedback_buffer.append({
            'query_id': query_id,
            'clicked_docs': clicked_doc_ids,
            'timestamp': time.time()
        })
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æ¨¡å‹
        if len(self.feedback_buffer) >= self.update_threshold:
            self._update_model()
    
    def _record_interaction(self, request: RerankRequest, results: List[SearchResult]):
        """è®°å½•äº¤äº’æ•°æ®"""
        # è¿™é‡Œå¯ä»¥è®°å½•æ›´è¯¦ç»†çš„äº¤äº’ä¿¡æ¯ç”¨äºåç»­å­¦ä¹ 
        pass
    
    def _update_model(self):
        """æ›´æ–°é‡æ’æ¨¡å‹"""
        logger.info(f"ğŸ”„ åŸºäº{len(self.feedback_buffer)}ä¸ªåé¦ˆæ›´æ–°æ¨¡å‹...")
        # å®é™…å®ç°ä¸­è¿™é‡Œä¼šä½¿ç”¨åœ¨çº¿å­¦ä¹ ç®—æ³•æ›´æ–°æ¨¡å‹å‚æ•°
        self.feedback_buffer = []  # æ¸…ç©ºç¼“å†²åŒº

class RerankingPipeline:
    """é‡æ’æµæ°´çº¿ - å·¥ä¸šç•Œæ ‡å‡†æ¶æ„"""
    
    def __init__(self):
        # æ„å»ºå¤šé˜¶æ®µé‡æ’æµæ°´çº¿
        self.pipeline = [
            ('pointwise', PointwiseReranker()),
            ('ensemble', EnsembleReranker()),
            ('online_learning', OnlineLearningReranker(EnsembleReranker()))
        ]
        logger.info("âœ… é‡æ’æµæ°´çº¿åˆå§‹åŒ–å®Œæˆ")
    
    def rerank_pipeline(self, request: RerankRequest, stage: str = 'production') -> List[SearchResult]:
        """æ‰§è¡Œé‡æ’æµæ°´çº¿"""
        logger.info(f"ğŸš€ æ‰§è¡Œ{stage}é‡æ’æµæ°´çº¿...")
        
        current_candidates = request.candidates
        timing_info = {}
        
        # æ ¹æ®é˜¶æ®µé€‰æ‹©åˆé€‚çš„é‡æ’å™¨
        if stage == 'quick':
            reranker = self.pipeline[0][1]  # å¿«é€Ÿæ¨¡å¼ï¼šä»…pointwise
        elif stage == 'balanced':
            reranker = self.pipeline[1][1]  # å¹³è¡¡æ¨¡å¼ï¼šé›†æˆé‡æ’
        else:  # productionæ¨¡å¼
            reranker = self.pipeline[2][1]  # ç”Ÿäº§æ¨¡å¼ï¼šåœ¨çº¿å­¦ä¹ 
        
        start_time = time.time()
        final_results = reranker.rerank(RerankRequest(
            query=request.query,
            candidates=current_candidates,
            context=request.context
        ))
        total_time = time.time() - start_time
        
        timing_info['total_time'] = total_time
        logger.info(f"âœ… é‡æ’æµæ°´çº¿æ‰§è¡Œå®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.4f}ç§’")
        
        return final_results, timing_info

class PerformanceEvaluator:
    """æ€§èƒ½è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.metrics_history = []
    
    def evaluate_reranking(self, ground_truth: List[int], predictions: List[SearchResult]) -> Dict[str, float]:
        """è¯„ä¼°é‡æ’æ€§èƒ½"""
        # æå–é¢„æµ‹çš„æ–‡æ¡£IDé¡ºåº
        predicted_order = [result.doc_id for result in predictions]
        
        # è®¡ç®—å„ç§æŒ‡æ ‡
        metrics = {
            'ndcg': self._calculate_ndcg(ground_truth, predicted_order),
            'precision_at_k': self._calculate_precision_at_k(ground_truth, predicted_order, k=5),
            'mrr': self._calculate_mrr(ground_truth, predicted_order),
            'map': self._calculate_map(ground_truth, predicted_order)
        }
        
        self.metrics_history.append(metrics)
        return metrics
    
    def _calculate_ndcg(self, ground_truth: List[int], predicted: List[int], k=10) -> float:
        """è®¡ç®—NDCG"""
        def dcg_score(items, relevance_dict, k):
            return sum((relevance_dict.get(item, 0) / np.log2(i + 2)) 
                      for i, item in enumerate(items[:k]))
        
        # æ„å»ºç›¸å…³æ€§å­—å…¸ï¼ˆground truthä¸­å‰é¢çš„æ›´ç›¸å…³ï¼‰
        relevance = {doc_id: len(ground_truth) - i 
                    for i, doc_id in enumerate(ground_truth)}
        
        # è®¡ç®—DCGå’ŒIDCG
        dcg = dcg_score(predicted, relevance, k)
        ideal_order = sorted(ground_truth, key=lambda x: relevance.get(x, 0), reverse=True)
        idcg = dcg_score(ideal_order, relevance, k)
        
        return dcg / idcg if idcg > 0 else 0
    
    def _calculate_precision_at_k(self, ground_truth: List[int], predicted: List[int], k=5) -> float:
        """è®¡ç®—Precision@K"""
        relevant_items = set(ground_truth[:k])
        predicted_items = set(predicted[:k])
        return len(relevant_items & predicted_items) / k if k > 0 else 0
    
    def _calculate_mrr(self, ground_truth: List[int], predicted: List[int]) -> float:
        """è®¡ç®—MRR"""
        for i, pred_item in enumerate(predicted):
            if pred_item in ground_truth:
                return 1.0 / (i + 1)
        return 0.0
    
    def _calculate_map(self, ground_truth: List[int], predicted: List[int]) -> float:
        """è®¡ç®—MAP"""
        relevant_items = set(ground_truth)
        ap_sum = 0.0
        hits = 0
        
        for i, pred_item in enumerate(predicted):
            if pred_item in relevant_items:
                hits += 1
                ap_sum += hits / (i + 1)
        
        return ap_sum / len(relevant_items) if relevant_items else 0

def demo_industry_reranking():
    """å·¥ä¸šçº§é‡æ’ä¼˜åŒ–æ¼”ç¤º"""
    logger.info("ğŸš€ å¯åŠ¨å·¥ä¸šçº§é‡æ’ä¼˜åŒ–æ¼”ç¤º")
    
    # å‡†å¤‡æ¼”ç¤ºæ•°æ®
    sample_documents = [
        SearchResult(0, "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œè¯­æ³•ç®€æ´æ˜“è¯»", 0.8, {'popularity': 0.9}),
        SearchResult(1, "Javaæ˜¯é¢å‘å¯¹è±¡çš„ç¼–ç¨‹è¯­è¨€ï¼Œå…·æœ‰è·¨å¹³å°ç‰¹æ€§", 0.7, {'popularity': 0.8}),
        SearchResult(2, "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„é‡è¦åˆ†æ”¯é¢†åŸŸ", 0.9, {'popularity': 0.95}),
        SearchResult(3, "æœºå™¨å­¦ä¹ æ˜¯å®ç°äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯æ–¹æ³•", 0.85, {'popularity': 0.85}),
        SearchResult(4, "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦å­é¢†åŸŸ", 0.82, {'popularity': 0.8}),
        SearchResult(5, "TensorFlowæ˜¯Googleå¼€å‘çš„æœºå™¨å­¦ä¹ æ¡†æ¶", 0.78, {'popularity': 0.75})
    ]
    
    # åˆ›å»ºé‡æ’æµæ°´çº¿
    pipeline = RerankingPipeline()
    evaluator = PerformanceEvaluator()
    
    # æµ‹è¯•æŸ¥è¯¢
    test_cases = [
        {
            'query': "Pythonç¼–ç¨‹è¯­è¨€çš„ç‰¹ç‚¹",
            'candidates': sample_documents[:4],
            'ground_truth': [0, 1, 2, 3]  # ç†æƒ³æ’åº
        },
        {
            'query': "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
            'candidates': sample_documents[2:],
            'ground_truth': [2, 3, 4, 5]  # ç†æƒ³æ’åº
        }
    ]
    
    # æ¼”ç¤ºä¸åŒé˜¶æ®µçš„é‡æ’æ•ˆæœ
    stages = ['quick', 'balanced', 'production']
    
    for stage in stages:
        logger.info(f"\n{'='*50}")
        logger.info(f"ğŸ“Š {stage.upper()} é˜¶æ®µé‡æ’æ¼”ç¤º")
        logger.info(f"{'='*50}")
        
        total_improvement = 0
        
        for i, test_case in enumerate(test_cases):
            request = RerankRequest(
                query=test_case['query'],
                candidates=test_case['candidates']
            )
            
            # æ‰§è¡Œé‡æ’
            results, timing = pipeline.rerank_pipeline(request, stage=stage)
            
            # è¯„ä¼°æ•ˆæœ
            metrics = evaluator.evaluate_reranking(test_case['ground_truth'], results)
            
            logger.info(f"\næµ‹è¯•ç”¨ä¾‹ {i+1}: {test_case['query']}")
            logger.info(f"å¤„ç†æ—¶é—´: {timing['total_time']:.4f}ç§’")
            logger.info(f"NDCG@10: {metrics['ndcg']:.4f}")
            logger.info(f"Precision@5: {metrics['precision_at_k']:.4f}")
            logger.info(f"MRR: {metrics['mrr']:.4f}")
            logger.info(f"MAP: {metrics['map']:.4f}")
            
            # æ˜¾ç¤ºæ’åºç»“æœ
            logger.info("æ’åºç»“æœ:")
            for j, result in enumerate(results[:3]):
                logger.info(f"  {j+1}. [åˆ†æ•°: {result.initial_score:.3f}] {result.content}")
            
            total_improvement += metrics['ndcg']
        
        avg_improvement = total_improvement / len(test_cases)
        logger.info(f"\nğŸ“ˆ {stage} é˜¶æ®µå¹³å‡NDCG: {avg_improvement:.4f}")
    
    # æ€»ç»“æ€§èƒ½å¯¹æ¯”
    logger.info(f"\n{'='*60}")
    logger.info("ğŸ† å·¥ä¸šçº§é‡æ’ä¼˜åŒ–æ¼”ç¤ºå®Œæˆ!")
    logger.info("ğŸ’¡ ä¸åŒé‡æ’ç­–ç•¥çš„é€‚ç”¨åœºæ™¯:")
    logger.info("   â€¢ Quick: å®æ—¶æ€§è¦æ±‚é«˜çš„åœºæ™¯")
    logger.info("   â€¢ Balanced: å¹³è¡¡æ•ˆæœå’Œæ€§èƒ½çš„é€šç”¨åœºæ™¯") 
    logger.info("   â€¢ Production: å¯¹æ•ˆæœè¦æ±‚æé«˜çš„ç”Ÿäº§ç¯å¢ƒ")
    logger.info(f"{'='*60}")

if __name__ == "__main__":
    demo_industry_reranking()