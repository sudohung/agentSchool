"""
CPU-onlyç¯å¢ƒä¸‹çš„é‡æ’ä¼˜åŒ–å®æˆ˜æ¼”ç¤º
CPU-Optimized Reranking Demo for Personal Computers
"""

import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
import time
import psutil
import logging
from typing import List, Dict, Tuple
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import gc

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class Document:
    """æ–‡æ¡£æ•°æ®ç»“æ„"""
    doc_id: int
    content: str
    features: Dict[str, float] = None
    
    def __post_init__(self):
        if self.features is None:
            self.features = {}

@dataclass
class RerankResult:
    """é‡æ’ç»“æœ"""
    doc_id: int
    content: str
    score: float
    rank: int

class CPUEfficientModels:
    """CPUå‹å¥½çš„æ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self):
        self.models = {}
        self.current_model = None
        self.model_configs = {
            'mini': {
                'name': 'all-MiniLM-L6-v2',
                'dim': 384,
                'memory_mb': 100,
                'speed_rating': 5  # 1-5è¯„åˆ†
            },
            'multilingual': {
                'name': 'paraphrase-multilingual-MiniLM-L12-v2', 
                'dim': 384,
                'memory_mb': 400,
                'speed_rating': 3
            },
            'bge_small': {
                'name': 'bge-small-en-v1.5',
                'dim': 384,
                'memory_mb': 150,
                'speed_rating': 4
            }
        }
    
    def load_model(self, model_type: str = 'mini'):
        """åŠ è½½CPUå‹å¥½çš„æ¨¡å‹"""
        if model_type not in self.model_configs:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
        
        config = self.model_configs[model_type]
        logger.info(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {config['name']} (å†…å­˜å ç”¨çº¦{config['memory_mb']}MB)")
        
        try:
            # æ£€æŸ¥å†…å­˜æ˜¯å¦å……è¶³
            available_memory = self._get_available_memory()
            if available_memory < config['memory_mb'] * 2:  # é¢„ç•™ä¸€äº›å†…å­˜
                logger.warning(f"âš ï¸ å†…å­˜å¯èƒ½ä¸è¶³ï¼Œå½“å‰å¯ç”¨: {available_memory}MB")
            
            self.current_model = SentenceTransformer(config['name'])
            self.models[model_type] = self.current_model
            logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def _get_available_memory(self) -> int:
        """è·å–å¯ç”¨å†…å­˜(MB)"""
        process = psutil.Process()
        memory_info = process.memory_info()
        available = psutil.virtual_memory().available / (1024 * 1024)
        return int(available)

class RuleBasedReranker:
    """åŸºäºè§„åˆ™çš„è½»é‡çº§é‡æ’å™¨"""
    
    def __init__(self):
        self.weights = {
            'bm25': 0.4,
            'popularity': 0.2,
            'recency': 0.2,
            'length_match': 0.2
        }
        logger.info("âœ… è§„åˆ™é‡æ’å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def rerank(self, query: str, candidates: List[Document], top_k: int = 10) -> List[RerankResult]:
        """åŸºäºè§„åˆ™çš„é‡æ’"""
        start_time = time.time()
        
        scored_candidates = []
        for doc in candidates:
            # è®¡ç®—å„é¡¹åˆ†æ•°
            bm25_score = self._calculate_bm25(query, doc.content)
            popularity_score = doc.features.get('popularity', 0.5)
            recency_score = doc.features.get('recency', 0.5)
            length_score = self._calculate_length_match(query, doc.content)
            
            # åŠ æƒèåˆ
            final_score = (
                self.weights['bm25'] * bm25_score +
                self.weights['popularity'] * popularity_score +
                self.weights['recency'] * recency_score +
                self.weights['length_match'] * length_score
            )
            
            scored_candidates.append((final_score, doc))
        
        # æ’åºå¹¶æˆªå–top-k
        scored_candidates.sort(key=lambda x: x[0], reverse=True)
        top_candidates = scored_candidates[:top_k]
        
        # æ„å»ºç»“æœ
        results = []
        for rank, (score, doc) in enumerate(top_candidates, 1):
            results.append(RerankResult(
                doc_id=doc.doc_id,
                content=doc.content,
                score=float(score),
                rank=rank
            ))
        
        processing_time = time.time() - start_time
        logger.info(f"ğŸ”„ è§„åˆ™é‡æ’å®Œæˆ: {len(candidates)}â†’{len(results)} é¡¹ï¼Œè€—æ—¶{processing_time:.4f}ç§’")
        
        return results
    
    def _calculate_bm25(self, query: str, document: str) -> float:
        """è½»é‡çº§BM25è®¡ç®—"""
        query_terms = query.lower().split()
        doc_terms = document.lower().split()
        
        if not query_terms or not doc_terms:
            return 0.0
        
        k1, b = 1.2, 0.75
        avg_doc_len = 100  # é¢„ä¼°å¹³å‡æ–‡æ¡£é•¿åº¦
        doc_len = len(doc_terms)
        
        score = 0.0
        for term in set(query_terms):  # å»é‡é¿å…é‡å¤è®¡ç®—
            tf = doc_terms.count(term)
            if tf > 0:
                # ç®€åŒ–ç‰ˆIDF
                idf = np.log(1000 / (tf + 0.5))
                numerator = tf * (k1 + 1)
                denominator = tf + k1 * (1 - b + b * doc_len / avg_doc_len)
                score += idf * numerator / denominator
        
        # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
        return min(1.0, score / len(query_terms))
    
    def _calculate_length_match(self, query: str, document: str) -> float:
        """é•¿åº¦åŒ¹é…åº¦è®¡ç®—"""
        query_len = len(query)
        doc_len = len(document)
        if query_len == 0 or doc_len == 0:
            return 0.0
        ratio = min(query_len, doc_len) / max(query_len, doc_len)
        return ratio

class LightweightMLReranker:
    """è½»é‡çº§æœºå™¨å­¦ä¹ é‡æ’å™¨"""
    
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.feature_names = ['word_overlap', 'length_ratio', 'popularity', 'simple_sim']
        self.is_trained = False
        logger.info("âœ… è½»é‡çº§MLé‡æ’å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def train(self, training_data: List[Tuple[str, List[Document], List[int]]]):
        """è®­ç»ƒè½»é‡çº§MLæ¨¡å‹"""
        logger.info("ğŸ‹ï¸ å¼€å§‹è®­ç»ƒè½»é‡çº§MLæ¨¡å‹...")
        
        X, y = [], []
        
        for query, docs, relevance_labels in training_data:
            for doc, label in zip(docs, relevance_labels):
                # æå–ç®€å•ç‰¹å¾
                features = self._extract_features(query, doc)
                X.append(features)
                # ç®€å•çš„äºŒåˆ†ç±»æ ‡ç­¾
                y.append(1 if label > 0 else 0)
        
        if len(X) > 10:  # éœ€è¦è¶³å¤Ÿçš„è®­ç»ƒæ ·æœ¬
            X_scaled = self.scaler.fit_transform(X)
            # ä½¿ç”¨é€»è¾‘å›å½’ï¼ˆè½»é‡çº§ï¼‰
            self.model = LogisticRegression(max_iter=1000, C=1.0)
            self.model.fit(X_scaled, y)
            self.is_trained = True
            logger.info(f"âœ… MLæ¨¡å‹è®­ç»ƒå®Œæˆï¼Œä½¿ç”¨äº†{len(X)}ä¸ªæ ·æœ¬")
        else:
            logger.warning("âš ï¸ è®­ç»ƒæ ·æœ¬ä¸è¶³ï¼Œæ— æ³•è®­ç»ƒMLæ¨¡å‹")
    
    def rerank(self, query: str, candidates: List[Document], top_k: int = 10) -> List[RerankResult]:
        """MLé‡æ’"""
        if not self.is_trained or not self.model:
            logger.warning("âš ï¸ MLæ¨¡å‹æœªè®­ç»ƒï¼Œä½¿ç”¨é»˜è®¤æ’åº")
            return self._default_rerank(candidates, top_k)
        
        start_time = time.time()
        
        # æå–ç‰¹å¾
        features_list = []
        for doc in candidates:
            features = self._extract_features(query, doc)
            features_list.append(features)
        
        # é¢„æµ‹æ¦‚ç‡
        features_scaled = self.scaler.transform(features_list)
        probabilities = self.model.predict_proba(features_scaled)[:, 1]  # æ­£ç±»æ¦‚ç‡
        
        # æ„å»ºç»“æœ
        scored_candidates = list(zip(probabilities, candidates))
        scored_candidates.sort(key=lambda x: x[0], reverse=True)
        
        results = []
        for rank, (score, doc) in enumerate(scored_candidates[:top_k], 1):
            results.append(RerankResult(
                doc_id=doc.doc_id,
                content=doc.content,
                score=float(score),
                rank=rank
            ))
        
        processing_time = time.time() - start_time
        logger.info(f"ğŸ”„ MLé‡æ’å®Œæˆ: {len(candidates)}â†’{len(results)} é¡¹ï¼Œè€—æ—¶{processing_time:.4f}ç§’")
        
        return results
    
    def _extract_features(self, query: str, doc: Document) -> List[float]:
        """æå–ç®€å•ç‰¹å¾"""
        query_words = set(query.lower().split())
        doc_words = set(doc.content.lower().split())
        
        # è¯æ±‡é‡å 
        word_overlap = len(query_words & doc_words)
        
        # é•¿åº¦æ¯”ä¾‹
        length_ratio = len(doc.content) / (len(query) + 1)
        
        # æµè¡Œåº¦
        popularity = doc.features.get('popularity', 0.5)
        
        # ç®€å•ç›¸ä¼¼åº¦
        simple_sim = len(query_words & doc_words) / len(query_words | doc_words) if (query_words | doc_words) else 0
        
        return [word_overlap, length_ratio, popularity, simple_sim]
    
    def _default_rerank(self, candidates: List[Document], top_k: int) -> List[RerankResult]:
        """é»˜è®¤æ’åºï¼ˆæŒ‰æ–‡æ¡£IDï¼‰"""
        results = []
        for rank, doc in enumerate(candidates[:top_k], 1):
            results.append(RerankResult(
                doc_id=doc.doc_id,
                content=doc.content,
                score=1.0/rank,  # ç®€å•ä½æ¬¡åˆ†æ•°
                rank=rank
            ))
        return results

class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨ - CPUä¼˜åŒ–"""
    
    def __init__(self, batch_size: int = 32, max_workers: int = 4):
        self.batch_size = batch_size
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        logger.info(f"âœ… æ‰¹é‡å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ (batch_size={batch_size}, workers={max_workers})")
    
    def batch_process(self, items: List, process_func, *args, **kwargs):
        """æ‰¹é‡å¤„ç†"""
        results = []
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(items), self.batch_size):
            batch = items[i:i + self.batch_size]
            
            # æ‰¹å†…å¹¶è¡Œå¤„ç†
            batch_results = list(self.executor.map(
                lambda item: process_func(item, *args, **kwargs), batch
            ))
            
            results.extend(batch_results)
            
            # å®šæœŸå†…å­˜æ¸…ç†
            if i % (self.batch_size * 4) == 0:
                self._cleanup_memory()
        
        return results
    
    def _cleanup_memory(self):
        """å†…å­˜æ¸…ç†"""
        gc.collect()

class CPURerankingPipeline:
    """CPUä¼˜åŒ–çš„é‡æ’æµæ°´çº¿"""
    
    def __init__(self, system_memory_mb: int = None):
        self.system_memory = system_memory_mb or self._detect_system_memory()
        self.models = CPUEfficientModels()
        self.batch_processor = BatchProcessor()
        self.rule_reranker = RuleBasedReranker()
        self.ml_reranker = LightweightMLReranker()
        
        # æ ¹æ®å†…å­˜è‡ªåŠ¨é€‰æ‹©é…ç½®
        self.config = self._select_optimal_config()
        logger.info(f"âœ… CPUé‡æ’æµæ°´çº¿åˆå§‹åŒ–å®Œæˆï¼Œé…ç½®: {self.config['name']}")
    
    def _detect_system_memory(self) -> int:
        """æ£€æµ‹ç³»ç»Ÿå†…å­˜"""
        total_memory = psutil.virtual_memory().total / (1024 * 1024)
        return int(total_memory)
    
    def _select_optimal_config(self) -> Dict:
        """æ ¹æ®ç³»ç»Ÿèµ„æºé€‰æ‹©æœ€ä¼˜é…ç½®"""
        configs = [
            {
                'name': 'å…¥é—¨çº§',
                'memory_threshold': 4000,
                'model': 'mini',
                'reranker': 'rule_based',
                'batch_size': 16,
                'max_candidates': 100
            },
            {
                'name': 'æ ‡å‡†çº§', 
                'memory_threshold': 8000,
                'model': 'bge_small',
                'reranker': 'hybrid',
                'batch_size': 32,
                'max_candidates': 500
            },
            {
                'name': 'é«˜æ€§èƒ½',
                'memory_threshold': 16000,
                'model': 'multilingual',
                'reranker': 'ml_based',
                'batch_size': 64,
                'max_candidates': 1000
            }
        ]
        
        # é€‰æ‹©é€‚åˆçš„é…ç½®
        for config in reversed(configs):
            if self.system_memory >= config['memory_threshold']:
                self.batch_processor.batch_size = config['batch_size']
                return config
        
        # é»˜è®¤è¿”å›æœ€ä½é…ç½®
        return configs[0]
    
    def rerank(self, query: str, candidates: List[Document], top_k: int = 10) -> List[RerankResult]:
        """æ‰§è¡ŒCPUä¼˜åŒ–çš„é‡æ’"""
        logger.info(f"ğŸš€ æ‰§è¡Œ{self.config['name']}é‡æ’é…ç½®")
        logger.info(f"   æŸ¥è¯¢: {query[:50]}...")
        logger.info(f"   å€™é€‰æ–‡æ¡£æ•°: {len(candidates)}")
        
        # æ ¹æ®é…ç½®é€‰æ‹©é‡æ’ç­–ç•¥
        strategy = self.config['reranker']
        
        if strategy == 'rule_based':
            return self.rule_reranker.rerank(query, candidates[:self.config['max_candidates']], top_k)
        elif strategy == 'ml_based':
            return self.ml_reranker.rerank(query, candidates[:self.config['max_candidates']], top_k)
        else:  # hybridæ··åˆç­–ç•¥
            return self._hybrid_rerank(query, candidates[:self.config['max_candidates']], top_k)
    
    def _hybrid_rerank(self, query: str, candidates: List[Document], top_k: int) -> List[RerankResult]:
        """æ··åˆé‡æ’ç­–ç•¥"""
        # ç¬¬ä¸€é˜¶æ®µï¼šè§„åˆ™è¿‡æ»¤
        intermediate_results = self.rule_reranker.rerank(
            query, candidates, top_k=min(100, len(candidates))
        )
        
        # ç¬¬äºŒé˜¶æ®µï¼šMLç²¾æ’
        filtered_docs = [
            Document(result.doc_id, result.content, {'initial_score': result.score})
            for result in intermediate_results
        ]
        
        return self.ml_reranker.rerank(query, filtered_docs, top_k)

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics_history = []
    
    def monitor_execution(self, func, *args, **kwargs):
        """ç›‘æ§å‡½æ•°æ‰§è¡Œæ€§èƒ½"""
        # å†…å­˜ç›‘æ§
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024
        
        # æ—¶é—´ç›‘æ§
        start_time = time.time()
        
        # æ‰§è¡Œå‡½æ•°
        result = func(*args, **kwargs)
        
        # è®°å½•æŒ‡æ ‡
        execution_time = time.time() - start_time
        final_memory = process.memory_info().rss / 1024 / 1024
        memory_used = final_memory - initial_memory
        
        metrics = {
            'execution_time': execution_time,
            'memory_used_mb': memory_used,
            'peak_memory_mb': final_memory,
            'timestamp': time.time()
        }
        
        self.metrics_history.append(metrics)
        return result, metrics
    
    def get_performance_summary(self):
        """è·å–æ€§èƒ½æ‘˜è¦"""
        if not self.metrics_history:
            return {}
        
        times = [m['execution_time'] for m in self.metrics_history]
        memories = [m['memory_used_mb'] for m in self.metrics_history]
        
        return {
            'avg_execution_time': np.mean(times),
            'max_execution_time': np.max(times),
            'avg_memory_usage': np.mean(memories),
            'max_memory_usage': np.max(memories),
            'total_executions': len(self.metrics_history)
        }

def create_sample_data(num_documents: int = 100) -> List[Document]:
    """åˆ›å»ºç¤ºä¾‹æ•°æ®"""
    sample_contents = [
        "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œè¯­æ³•ç®€æ´æ˜“è¯»ï¼Œé€‚åˆåˆå­¦è€…å­¦ä¹ ",
        "Javaæ˜¯é¢å‘å¯¹è±¡çš„ç¼–ç¨‹è¯­è¨€ï¼Œå…·æœ‰è·¨å¹³å°ç‰¹æ€§ï¼Œå¹¿æ³›åº”ç”¨ä¼ä¸šå¼€å‘",
        "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„é‡è¦åˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›é€ æ™ºèƒ½æœºå™¨ç³»ç»Ÿ",
        "æœºå™¨å­¦ä¹ æ˜¯å®ç°äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ",
        "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é¢†åŸŸï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿäººè„‘å­¦ä¹ è¿‡ç¨‹",
        "TensorFlowæ˜¯Googleå¼€å‘çš„å¼€æºæœºå™¨å­¦ä¹ æ¡†æ¶ï¼ŒåŠŸèƒ½å¼ºå¤§æ˜“ç”¨",
        "PyTorchæ˜¯Facebookå¼€å‘çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ·±å—ç ”ç©¶è€…å–œçˆ±",
        "æ•°æ®åˆ†ææ˜¯ä»å¤§é‡æ•°æ®ä¸­æå–æœ‰ä»·å€¼ä¿¡æ¯çš„è¿‡ç¨‹å’ŒæŠ€æœ¯",
        "äº‘è®¡ç®—æä¾›æŒ‰éœ€çš„è®¡ç®—èµ„æºå…±äº«æ± ï¼ŒåŒ…æ‹¬æœåŠ¡å™¨ã€å­˜å‚¨ã€ç½‘ç»œç­‰",
        "åŒºå—é“¾æ˜¯ä¸€ç§åˆ†å¸ƒå¼è´¦æœ¬æŠ€æœ¯ï¼Œå…·æœ‰å»ä¸­å¿ƒåŒ–å’Œä¸å¯ç¯¡æ”¹çš„ç‰¹ç‚¹"
    ]
    
    documents = []
    for i in range(num_documents):
        # å¾ªç¯ä½¿ç”¨æ ·æœ¬å†…å®¹å¹¶æ·»åŠ å˜åŒ–
        base_content = sample_contents[i % len(sample_contents)]
        # æ·»åŠ ä¸€äº›å˜åŒ–ä½¿å†…å®¹æ›´ä¸°å¯Œ
        variations = [
            f"{base_content} è¿™æ˜¯éå¸¸é‡è¦çš„æŠ€æœ¯æ¦‚å¿µã€‚",
            f"{base_content} åœ¨ç°ä»£æŠ€æœ¯å‘å±•ä¸­æ‰®æ¼”å…³é”®è§’è‰²ã€‚",
            f"{base_content} æœ‰ç€å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚"
        ]
        content = variations[(i // len(sample_contents)) % len(variations)]
        
        # æ·»åŠ éšæœºç‰¹å¾
        features = {
            'popularity': np.random.uniform(0.3, 1.0),
            'recency': np.random.uniform(0.1, 1.0)
        }
        
        documents.append(Document(doc_id=i, content=content, features=features))
    
    return documents

def demo_cpu_reranking():
    """CPUé‡æ’ä¼˜åŒ–æ¼”ç¤º"""
    logger.info("ğŸš€ å¯åŠ¨CPU-onlyé‡æ’ä¼˜åŒ–æ¼”ç¤º")
    logger.info(f"ğŸ’» ç³»ç»Ÿå†…å­˜: {psutil.virtual_memory().total / (1024**3):.1f}GB")
    
    # åˆ›å»ºæ¼”ç¤ºæ•°æ®
    sample_documents = create_sample_data(200)
    logger.info(f"ğŸ“„ å‡†å¤‡äº† {len(sample_documents)} ä¸ªç¤ºä¾‹æ–‡æ¡£")
    
    # åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨
    monitor = PerformanceMonitor()
    
    # æµ‹è¯•ä¸åŒé…ç½®
    test_configs = ['å…¥é—¨çº§', 'æ ‡å‡†çº§', 'é«˜æ€§èƒ½']
    test_queries = [
        "Pythonç¼–ç¨‹è¯­è¨€çš„ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Ÿ",
        "æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"
    ]
    
    for config_name in test_configs:
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“Š æµ‹è¯• {config_name} é…ç½®")
        logger.info(f"{'='*60}")
        
        # åˆ›å»ºå¯¹åº”é…ç½®çš„æµæ°´çº¿
        pipeline = CPURerankingPipeline()
        
        # ä¸´æ—¶ä¿®æ”¹é…ç½®ä»¥æµ‹è¯•ä¸åŒç­–ç•¥
        if config_name == 'å…¥é—¨çº§':
            pipeline.config['reranker'] = 'rule_based'
        elif config_name == 'æ ‡å‡†çº§':
            pipeline.config['reranker'] = 'hybrid'
        else:  # é«˜æ€§èƒ½
            pipeline.config['reranker'] = 'ml_based'
        
        total_time = 0
        total_memory = 0
        
        for i, query in enumerate(test_queries):
            # ç›‘æ§æ‰§è¡Œæ€§èƒ½
            results, metrics = monitor.monitor_execution(
                pipeline.rerank, query, sample_documents, 10
            )
            
            total_time += metrics['execution_time']
            total_memory += metrics['memory_used_mb']
            
            logger.info(f"\næŸ¥è¯¢ {i+1}: {query}")
            logger.info(f"å¤„ç†æ—¶é—´: {metrics['execution_time']:.4f}ç§’")
            logger.info(f"å†…å­˜ä½¿ç”¨: {metrics['memory_used_mb']:.1f}MB")
            logger.info(f"è¿”å›ç»“æœæ•°: {len(results)}")
            
            # æ˜¾ç¤ºå‰3ä¸ªç»“æœ
            logger.info("Top 3 ç»“æœ:")
            for result in results[:3]:
                logger.info(f"  {result.rank}. [åˆ†æ•°:{result.score:.3f}] {result.content[:60]}...")
        
        avg_time = total_time / len(test_queries)
        avg_memory = total_memory / len(test_queries)
        logger.info(f"\nğŸ“ˆ {config_name} é…ç½®æ±‡æ€»:")
        logger.info(f"   å¹³å‡å¤„ç†æ—¶é—´: {avg_time:.4f}ç§’")
        logger.info(f"   å¹³å‡å†…å­˜ä½¿ç”¨: {avg_memory:.1f}MB")
    
    # è¾“å‡ºæ€»ä½“æ€§èƒ½æŠ¥å‘Š
    summary = monitor.get_performance_summary()
    logger.info(f"\n{'='*60}")
    logger.info("ğŸ† CPUé‡æ’ä¼˜åŒ–æ¼”ç¤ºå®Œæˆ!")
    logger.info(f"ğŸ“Š æ€»ä½“æ€§èƒ½æ‘˜è¦:")
    logger.info(f"   æ€»æ‰§è¡Œæ¬¡æ•°: {summary['total_executions']}")
    logger.info(f"   å¹³å‡æ‰§è¡Œæ—¶é—´: {summary['avg_execution_time']:.4f}ç§’")
    logger.info(f"   æœ€å¤§å†…å­˜ä½¿ç”¨: {summary['max_memory_usage']:.1f}MB")
    logger.info(f"   å¹³å‡å†…å­˜ä½¿ç”¨: {summary['avg_memory_usage']:.1f}MB")
    logger.info(f"{'='*60}")
    
    # å®ç”¨å»ºè®®
    logger.info(f"\nğŸ’¡ å®ç”¨å»ºè®®:")
    logger.info(f"   â€¢ å†…å­˜â‰¤4GB: æ¨èå…¥é—¨çº§é…ç½®(è§„åˆ™é‡æ’)")
    logger.info(f"   â€¢ å†…å­˜4-8GB: æ¨èæ ‡å‡†çº§é…ç½®(æ··åˆé‡æ’)") 
    logger.info(f"   â€¢ å†…å­˜â‰¥8GB: å¯ä½¿ç”¨é«˜æ€§èƒ½é…ç½®(MLé‡æ’)")
    logger.info(f"   â€¢ å…³é”®æ˜¯å®ç”¨æ€§è€Œéç†è®ºæœ€ä¼˜!")

if __name__ == "__main__":
    demo_cpu_reranking()