# ğŸ“š å‘é‡æ•°æ®åº“å’ŒåµŒå…¥æ¨¡å‹æ–°æ‰‹æŒ‡å—

æ¬¢è¿å­¦ä¹ å‘é‡æ•°æ®åº“å’ŒåµŒå…¥æ¨¡å‹çš„åŸºç¡€çŸ¥è¯†ï¼æœ¬æ–‡æ¡£å°†ç”¨ç®€å•æ˜“æ‡‚çš„æ–¹å¼ä»‹ç»è¿™äº›æ¦‚å¿µåŠå…¶åœ¨ AI åº”ç”¨ä¸­çš„ä½¿ç”¨ã€‚

## ğŸ¯ ä»€ä¹ˆæ˜¯å‘é‡æ•°æ®åº“ï¼Ÿ

### ç®€å•ç†è§£
æƒ³è±¡ä½ åœ¨å›¾ä¹¦é¦†æ‰¾ä¹¦ï¼š
- **ä¼ ç»Ÿæ•°æ®åº“**ï¼šåƒæŒ‰ä¹¦åæˆ–ä½œè€…æŸ¥æ‰¾ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
- **å‘é‡æ•°æ®åº“**ï¼šåƒæŒ‰å†…å®¹ç›¸ä¼¼åº¦æ‰¾ä¹¦ï¼ˆè¯­ä¹‰æœç´¢ï¼‰

### æ ¸å¿ƒæ¦‚å¿µ
```
æ–‡æœ¬ â†’ åµŒå…¥æ¨¡å‹ â†’ æ•°å­—å‘é‡ â†’ å‘é‡æ•°æ®åº“ â†’ ç›¸ä¼¼åº¦æœç´¢
```

## ğŸ§  åµŒå…¥æ¨¡å‹ explained

### ä»€ä¹ˆæ˜¯åµŒå…¥ï¼ˆEmbeddingï¼‰ï¼Ÿ
åµŒå…¥å°±æ˜¯æŠŠæ–‡å­—è½¬æ¢æˆæ•°å­—å‘é‡çš„è¿‡ç¨‹ï¼š

```
"ä½ å¥½ä¸–ç•Œ" â†’ [0.23, -0.45, 0.67, 0.12, ...] (æ•°ç™¾ä¸ªæ•°å­—)
"Hello World" â†’ [0.18, -0.39, 0.71, 0.09, ...] (æ•°ç™¾ä¸ªæ•°å­—)
```

### å¸¸ç”¨åµŒå…¥æ¨¡å‹å¯¹æ¯”

| æ¨¡å‹åç§° | è¯­è¨€æ”¯æŒ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|---------|---------|------|----------|
| **paraphrase-multilingual-MiniLM-L12-v2** | å¤šè¯­è¨€ | å¹³è¡¡æ€§èƒ½ | é€šç”¨å¤šè¯­è¨€åº”ç”¨ |
| **all-MiniLM-L6-v2** | è‹±æ–‡ä¸ºä¸» | è½»é‡å¿«é€Ÿ | è‹±æ–‡åº”ç”¨ã€èµ„æºå—é™ç¯å¢ƒ |
| **bge-small-zh-v1.5** | ä¸­æ–‡ä¼˜åŒ– | ä¸­æ–‡æ•ˆæœä½³ | ä¸­æ–‡è¯­ä¹‰æœç´¢ |

## ğŸ› ï¸ å®é™…ä½¿ç”¨æ•™ç¨‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…å¿…è¦ä¾èµ–
pip install sentence-transformers faiss-cpu numpy
```

### 2. åŸºç¡€ä½¿ç”¨ç¤ºä¾‹

```python
from sentence_transformers import SentenceTransformer
import numpy as np

# åŠ è½½åµŒå…¥æ¨¡å‹
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# æ–‡æœ¬è½¬åµŒå…¥å‘é‡
texts = ["äººå·¥æ™ºèƒ½å¾ˆæœ‰è¶£", "æœºå™¨å­¦ä¹ å¾ˆæœ‰ç”¨", "æˆ‘å–œæ¬¢ç¼–ç¨‹"]
embeddings = model.encode(texts)

print(f"å‘é‡ç»´åº¦: {len(embeddings[0])}")  # é€šå¸¸æ˜¯ 384 æˆ– 768 ç»´
print(f"å‘é‡æ•°é‡: {len(embeddings)}")
```

### 3. ä½¿ç”¨ FAISS å‘é‡æ•°æ®åº“

```python
import faiss

# åˆ›å»ºå‘é‡ç´¢å¼•
dimension = len(embeddings[0])  # å‘é‡ç»´åº¦
index = faiss.IndexFlatL2(dimension)  # L2 è·ç¦»ç´¢å¼•

# æ·»åŠ å‘é‡åˆ°æ•°æ®åº“
index.add(np.array(embeddings))

# æŸ¥è¯¢ç›¸ä¼¼å‘é‡
query_text = "æˆ‘å¯¹AIæŠ€æœ¯æ„Ÿå…´è¶£"
query_vector = model.encode([query_text])

# æœç´¢æœ€ç›¸ä¼¼çš„ 2 ä¸ªç»“æœ
distances, indices = index.search(np.array([query_vector]), k=2)

print("æœ€ç›¸ä¼¼çš„æ–‡æœ¬:")
for i, idx in enumerate(indices[0]):
    print(f"{i+1}. {texts[idx]} (è·ç¦»: {distances[0][i]:.2f})")
```

## ğŸ® å®é™…åº”ç”¨åœºæ™¯

### åœºæ™¯ 1ï¼šæ™ºèƒ½é—®ç­”ç³»ç»Ÿ
```python
# çŸ¥è¯†åº“
knowledge_base = [
    "Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€",
    "Javaä¹Ÿæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€", 
    "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯",
    "æœºå™¨å­¦ä¹ æ˜¯AIçš„é‡è¦ç»„æˆéƒ¨åˆ†"
]

# æ„å»ºå‘é‡æ•°æ®åº“
embeddings = model.encode(knowledge_base)
index = faiss.IndexFlatL2(len(embeddings[0]))
index.add(np.array(embeddings))

# é—®ç­”åŠŸèƒ½
def answer_question(question):
    query_vector = model.encode([question])
    distances, indices = index.search(np.array([query_vector]), k=1)
    return knowledge_base[indices[0][0]]

# æµ‹è¯•
print(answer_question("ä»€ä¹ˆæ˜¯Pythonï¼Ÿ"))  # åº”è¯¥è¿”å›å…³äºPythonçš„å›ç­”
```

### åœºæ™¯ 2ï¼šæ–‡æ¡£ç›¸ä¼¼åº¦æœç´¢
```python
# æ–‡æ¡£å»é‡ç¤ºä¾‹
documents = [
    "äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•è¿…é€Ÿ",
    "AIæŠ€æœ¯è¿›æ­¥å¾ˆå¿«",  # ä¸ä¸Šé¢å¾ˆç›¸ä¼¼
    "ä»Šå¤©å¤©æ°”å¾ˆå¥½",
    "æœºå™¨å­¦ä¹ ç®—æ³•ä¸æ–­ä¼˜åŒ–"
]

# æ‰¾å‡ºç›¸ä¼¼æ–‡æ¡£
def find_similar_docs(threshold=0.8):
    embeddings = model.encode(documents)
    similar_pairs = []
    
    for i in range(len(documents)):
        for j in range(i+1, len(documents)):
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = np.dot(embeddings[i], embeddings[j]) / \
                        (np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[j]))
            if similarity > threshold:
                similar_pairs.append((i, j, similarity))
    
    return similar_pairs

similar_docs = find_similar_docs()
print("ç›¸ä¼¼æ–‡æ¡£å¯¹:", similar_docs)
```

## ğŸ”§ è¿›é˜¶æŠ€å·§

### 1. æ€§èƒ½ä¼˜åŒ–
```python
# æ‰¹é‡å¤„ç†æé«˜æ•ˆç‡
large_texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", ..., "æ–‡æœ¬1000"]
batch_size = 32

# åˆ†æ‰¹ç¼–ç 
all_embeddings = []
for i in range(0, len(large_texts), batch_size):
    batch = large_texts[i:i+batch_size]
    batch_embeddings = model.encode(batch)
    all_embeddings.extend(batch_embeddings)
```

### 2. ä¸åŒè·ç¦»åº¦é‡
```python
# ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆæ›´é€‚åˆæ–‡æœ¬ï¼‰
index_cosine = faiss.IndexFlatIP(dimension)  # å†…ç§¯
faiss.normalize_L2(embeddings)  # å½’ä¸€åŒ–
index_cosine.add(embeddings)

# æ¬§æ°è·ç¦»ï¼ˆé»˜è®¤ï¼‰
index_l2 = faiss.IndexFlatL2(dimension)
index_l2.add(embeddings)
```

### 3. æŒä¹…åŒ–å­˜å‚¨
```python
# ä¿å­˜ç´¢å¼•
faiss.write_index(index, "my_vector_index.index")

# åŠ è½½ç´¢å¼•
loaded_index = faiss.read_index("my_vector_index.index")
```

## ğŸš¨ å¸¸è§é—®é¢˜è§£ç­”

### Q: å‘é‡ç»´åº¦è¶Šé«˜è¶Šå¥½å—ï¼Ÿ
**A:** ä¸ä¸€å®šã€‚ç»´åº¦é«˜æ„å‘³ç€æ›´ç²¾ç¡®ä½†è®¡ç®—æˆæœ¬æ›´é«˜ã€‚ä¸€èˆ¬ 384-768 ç»´å°±è¶³å¤Ÿå¤§å¤šæ•°åº”ç”¨ã€‚

### Q: å¦‚ä½•é€‰æ‹©åˆé€‚çš„åµŒå…¥æ¨¡å‹ï¼Ÿ
**A:** 
- ä¸­æ–‡ä¸ºä¸» â†’ bge-small-zh-v1.5
- å¤šè¯­è¨€éœ€æ±‚ â†’ paraphrase-multilingual-MiniLM-L12-v2  
- è‹±æ–‡ä¸”è¿½æ±‚é€Ÿåº¦ â†’ all-MiniLM-L6-v2

### Q: å‘é‡æ•°æ®åº“æ¯”ä¼ ç»Ÿæ•°æ®åº“æ…¢å—ï¼Ÿ
**A:** æ°æ°ç›¸åï¼å¯¹äºç›¸ä¼¼åº¦æœç´¢ï¼Œå‘é‡æ•°æ®åº“æ¯”ä¼ ç»Ÿæ•°æ®åº“å¿«å¾ˆå¤šå€ã€‚

### Q: éœ€è¦å¤šå°‘æ•°æ®æ‰èƒ½çœ‹åˆ°æ•ˆæœï¼Ÿ
**A:** ä¸€èˆ¬æ¥è¯´ï¼Œå‡ ååˆ°å‡ ç™¾æ¡æ–‡æœ¬å°±èƒ½çœ‹åˆ°æ˜æ˜¾çš„ç›¸ä¼¼åº¦æœç´¢æ•ˆæœã€‚

## ğŸ¯ å®è·µç»ƒä¹ 

### ç»ƒä¹  1ï¼šæ„å»ºä¸ªäººç¬”è®°æœç´¢ç³»ç»Ÿ
```python
# TODO: å®ç°ä¸€ä¸ªå¯ä»¥æœç´¢ä¸ªäººç¬”è®°çš„ç³»ç»Ÿ
# æç¤ºï¼šå°†ç¬”è®°å†…å®¹å­˜å…¥å‘é‡æ•°æ®åº“ï¼Œå®ç°è¯­ä¹‰æœç´¢åŠŸèƒ½
```

### ç»ƒä¹  2ï¼šç”µå½±æ¨èç³»ç»Ÿ
```python
# TODO: åŸºäºç”µå½±æè¿°çš„ç›¸ä¼¼åº¦æ¨èç³»ç»Ÿ
# æç¤ºï¼šä½¿ç”¨ç”µå½±ç®€ä»‹ä½œä¸ºæ–‡æœ¬ï¼Œæ‰¾å‡ºç›¸ä¼¼çš„ç”µå½±
```

## ğŸ“š è¿›ä¸€æ­¥å­¦ä¹ èµ„æº

- [Sentence Transformers å®˜æ–¹æ–‡æ¡£](https://www.sbert.net/)
- [FAISS å®˜æ–¹æ•™ç¨‹](https://github.com/facebookresearch/faiss/wiki)
- [å‘é‡æ•°æ®åº“æ¯”è¾ƒ](https://weaviate.io/blog/vector-search-explained)

---

ğŸ‰ **æ­å–œå®ŒæˆåŸºç¡€å­¦ä¹ ï¼** ç°åœ¨ä½ å¯ä»¥å¼€å§‹æ„å»ºè‡ªå·±çš„å‘é‡æœç´¢åº”ç”¨äº†ï¼

*æœ‰ä»»ä½•é—®é¢˜æ¬¢è¿åœ¨ Issues ä¸­æé—®*